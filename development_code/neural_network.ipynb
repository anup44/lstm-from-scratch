{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tqdm\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense(object):\n",
    "    def __init__(self, out_dim, inp_dim=None, reg=0.001):\n",
    "        self.W = None\n",
    "        self.b = np.random.normal(0, 0.2, out_dim)\n",
    "        self.reg = reg\n",
    "        self.out_dim = out_dim\n",
    "        self.inp_dim = inp_dim\n",
    "        self.inp = None\n",
    "        self.next = None\n",
    "        self.momment1 = None\n",
    "        self.momment2 = None\n",
    "        self.momment_b1 = np.zeros_like(self.b)\n",
    "        self.momment_b2 = np.zeros_like(self.b)\n",
    "        if inp_dim:\n",
    "            # self.W = np.random.normal(0, 1, (self.out_dim, self.inp_dim))\n",
    "            self.W = np.random.normal(0, np.sqrt(2.0/(self.out_dim + self.inp_dim)), (self.out_dim, self.inp_dim))\n",
    "            self.momment1 = np.zeros_like(self.W)\n",
    "            self.momment2 = np.zeros_like(self.W)\n",
    "\n",
    "    def __call__(self, inp=None):\n",
    "        self.inp = inp\n",
    "        inp.next = self\n",
    "        self.inp_dim = inp.out_dim\n",
    "        # self.W = np.random.normal(0, 1, (self.out_dim, self.inp_dim))\n",
    "        self.W = np.random.normal(0, np.sqrt(2.0/(self.out_dim + self.inp_dim)), (self.out_dim, self.inp_dim))\n",
    "        self.momment1 = np.zeros_like(self.W)\n",
    "        self.momment2 = np.zeros_like(self.W)\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (self.__class__.__name__ + ' output: ' + str(self.out_dim) + ' input: ' + str(self.inp_dim))\n",
    "\n",
    "    def forward(self, X, y=None, W=None, b=None):\n",
    "        self.X = X\n",
    "        if len(self.X.shape) > 2:\n",
    "            X = X.reshape(-1, X.shape[-1])\n",
    "        if not W:\n",
    "            W = self.W\n",
    "        if not b:\n",
    "            b = self.b\n",
    "        out = np.dot(W, X.T).T + b\n",
    "\n",
    "        if len(self.X.shape) > 2:\n",
    "            out = out.reshape(*self.X.shape[:-1], self.out_dim)\n",
    "        return out # logits\n",
    "    \n",
    "    def backward(self, dO):\n",
    "        dO_shape = dO.shape\n",
    "        X = self.X\n",
    "        if len(dO_shape) > 2 or len(self.X.shape) > 2:\n",
    "            dO = dO.reshape(-1, dO.shape[-1])\n",
    "            X = X.reshape(-1, self.X.shape[-1])\n",
    "        \n",
    "        dW = np.sum(X[:, np.newaxis, :] * dO[:, :, np.newaxis], axis=0) + self.reg*self.W\n",
    "        dX = np.sum(self.W[:, :, np.newaxis] * dO.T[:, np.newaxis, :], axis=0).T\n",
    "        db = np.sum(dO, axis=0)\n",
    "        \n",
    "        if len(self.X.shape) > 2:\n",
    "            dX = dX.reshape(self.X.shape)\n",
    "        return (dX, dW, db)\n",
    "    \n",
    "class LSTM(object):\n",
    "    # inputs will include a mask\n",
    "    # if return_seq is false, the last output will be determined using the mask\n",
    "    def __init__(self, hidden_units, inp_dim=None, return_seq=True, return_mask=False, reg=0.001):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.reg = reg\n",
    "        self.out_dim = hidden_units\n",
    "        self.inp_dim = inp_dim\n",
    "        self.next = None\n",
    "        self.return_seq = return_seq\n",
    "        self.return_mask = return_mask\n",
    "        self.b = np.random.normal(0, 0.2, (4, self.out_dim))\n",
    "        self.bf = self.b[0]\n",
    "        self.bi = self.b[1]\n",
    "        self.bo = self.b[2]\n",
    "        self.bc = self.b[3]\n",
    "        self.momment1 = None\n",
    "        self.momment2 = None\n",
    "        self.momment_b1 = np.zeros_like(self.b)\n",
    "        self.momment_b2 = np.zeros_like(self.b)\n",
    "        if inp_dim:\n",
    "            self.W = np.random.normal(0, np.sqrt(2.0/(2 * self.out_dim + self.inp_dim)), (self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "            self.Wf = self.W[:self.out_dim, :]\n",
    "            self.Wi = self.W[self.out_dim: 2 * self.out_dim, :]\n",
    "            self.Wo = self.W[2 * self.out_dim: 3 * self.out_dim, :]\n",
    "            self.Wc = self.W[3 * self.out_dim:, :]\n",
    "            self.b = np.random.normal(0, 0.2, 4 * self.out_dim)\n",
    "            self.momment1 = np.zeros((self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "            self.momment2 = np.zeros((self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "\n",
    "    \n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        inp.next = self\n",
    "        self.inp_dim = inp.out_dim\n",
    "        self.W = np.random.normal(0, np.sqrt(2.0/(2 * self.out_dim + self.inp_dim)), (self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "        self.Wf = self.W[:self.out_dim, :]\n",
    "        self.Wi = self.W[self.out_dim: 2 * self.out_dim, :]\n",
    "        self.Wo = self.W[2 * self.out_dim: 3 * self.out_dim, :]\n",
    "        self.Wc = self.W[3 * self.out_dim:, :]\n",
    "        self.momment1 = np.zeros((self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "        self.momment2 = np.zeros((self.out_dim * 4, self.inp_dim + self.out_dim))\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (self.__class__.__name__ + ' output: ' + str(self.out_dim) + ' input: ' + str(self.inp_dim))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(X):\n",
    "        return 1.0/(1.0 + np.exp(-1 * X))\n",
    "    \n",
    "    def forward(self, X, y=None, W=None, b=None):\n",
    "        #W = out x in\n",
    "        self.Wf = self.W[:self.out_dim, :]\n",
    "        self.Wi = self.W[self.out_dim: 2 * self.out_dim, :]\n",
    "        self.Wo = self.W[2 * self.out_dim: 3 * self.out_dim, :]\n",
    "        self.Wc = self.W[3 * self.out_dim:, :]\n",
    "\n",
    "        self.bf = self.b[0]\n",
    "        self.bi = self.b[1]\n",
    "        self.bo = self.b[2]\n",
    "        self.bc = self.b[3]\n",
    "\n",
    "        if isinstance(X, dict):\n",
    "            X_ = X\n",
    "            X = X_['input_ids']\n",
    "            mask = X_.get('seq_lens', np.array(X.shape[1]).repeat(X.shape[0]))\n",
    "        else:\n",
    "            mask = np.array(X.shape[1]).repeat(X.shape[0])\n",
    "        fg = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        ig = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        og = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        pstate = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        state = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        out = np.zeros((X.shape[0], X.shape[1] + 1, self.out_dim))\n",
    "        X = np.concatenate([np.zeros((X.shape[0], 1, X.shape[2])), X], axis=1) # shape of X is (N, T, d)\n",
    "        self.X = X\n",
    "        self.mask = mask\n",
    "        \n",
    "        for t in range(1, X.shape[1]):\n",
    "            fg[:, t, :] = self.sigmoid(np.dot(np.hstack([out[:, t - 1, :], X[:, t, :]]), self.Wf.T) + self.bf)\n",
    "            ig[:, t, :] = self.sigmoid(np.dot(np.hstack([out[:, t - 1, :], X[:, t, :]]), self.Wi.T) + self.bi)\n",
    "            pstate[:, t, :] = np.tanh(np.dot(np.hstack([out[:, t - 1, :], X[:, t, :]]), self.Wc.T) + self.bc)\n",
    "            state[:, t, :] = fg[:, t, :] * state[:, t - 1, :] + ig[:, t, :] * pstate[:, t, :]\n",
    "            og[:, t, :] = self.sigmoid(np.dot(np.hstack([out[:, t - 1, :], X[:, t, :]]), self.Wo.T) + self.bo)\n",
    "            out[:, t, :] = og[:, t, :] * np.tanh(state[:, t, :])\n",
    "        self.fg = fg\n",
    "        self.ig = ig\n",
    "        self.og = og\n",
    "        self.pstate = pstate\n",
    "        self.state = state\n",
    "        self.out = out\n",
    "        out = out[:, 1:, :] if self.return_seq else out[np.arange(X.shape[0]), mask, :]\n",
    "        # return out[:, 1:, :] if self.return_seq else out[np.arange(X.shape[0]), mask, :]\n",
    "        return {'input_ids': out, 'seq_lens': mask} if self.return_mask else out\n",
    "\n",
    "    def backward(self, dO):\n",
    "        if not self.return_seq:\n",
    "            dO_ = np.zeros_like(self.out)\n",
    "            dO_[np.arange(self.X.shape[0]), self.mask, :] = dO\n",
    "            dO = dO_\n",
    "        else:\n",
    "            dO = np.concatenate([np.zeros((dO.shape[0], 1, dO.shape[2])), dO], axis=1)\n",
    "        # print (dO)\n",
    "        dstate = np.zeros_like(self.state)\n",
    "        dWo = np.zeros_like(self.Wo)\n",
    "        dWi = np.zeros_like(self.Wi)\n",
    "        dWf = np.zeros_like(self.Wf)\n",
    "        dWc = np.zeros_like(self.Wc)\n",
    "        dbf = np.zeros_like(self.bf)\n",
    "        dbi = np.zeros_like(self.bi)\n",
    "        dbo = np.zeros_like(self.bo)\n",
    "        dbc = np.zeros_like(self.bc)\n",
    "        dX = np.zeros_like(self.X)\n",
    "        mask = np.arange(self.X.shape[1]) > self.mask[:, np.newaxis]\n",
    "        for t in range(self.X.shape[1] - 1, 0, -1):\n",
    "            dstate[:, t, :] += dO[:, t, :] * self.og[:, t, :] * (1 - np.square(np.tanh(self.state[:, t, :])))\n",
    "            dstate[:, t - 1, :] = dstate[:, t, :] * self.fg[:, t, :]\n",
    "            \n",
    "            dWo += np.dot((dO[:, t, :] * np.tanh(self.state[:, t, :]) * self.og[:, t, :] * (1 - self.og[:, t, :])).T, \n",
    "                          np.hstack([self.out[:, t, :], self.X[:, t, :]]))\n",
    "            dWi += np.dot((dstate[:, t, :] * self.pstate[:, t, :] * self.ig[:, t, :] * (1 - self.ig[:, t, :])).T, \n",
    "                          np.hstack([self.out[:, t, :], self.X[:, t, :]]))\n",
    "            dWf += np.dot((dstate[:, t, :] * self.state[:, t - 1, :] * self.fg[:, t, :] * (1 - self.fg[:, t, :])).T, \n",
    "                          np.hstack([self.out[:, t, :], self.X[:, t, :]]))\n",
    "            dWc += np.dot((dstate[:, t, :] * self.ig[:, t, :] * (1 - np.square(self.pstate[:, t, :]))).T,\n",
    "                          np.hstack([self.out[:, t, :], self.X[:, t, :]]))\n",
    "            \n",
    "            dO[:, t - 1, :] += np.dot(dstate[:, t, :] * self.ig[:, t, :] * (1 - np.square(self.pstate[:, t, :])), self.Wc[:, :self.out_dim])\\\n",
    "                            + np.dot(dstate[:, t, :] * self.state[:, t - 1, :] * self.fg[:, t, :] * (1 - self.fg[:, t, :]), self.Wf[:, :self.out_dim])\\\n",
    "                            + np.dot(dstate[:, t, :] * self.pstate[:, t, :] * self.ig[:, t, :] * (1 - self.ig[:, t, :]), self.Wi[:, :self.out_dim])\\\n",
    "                            + np.dot(dO[:, t, :] * np.tanh(self.state[:, t, :]) * self.og[:, t, :] * (1 - self.og[:, t, :]), self.Wo[:, :self.out_dim])\n",
    "            \n",
    "            dX[:, t, :] = np.dot(dstate[:, t, :] * self.ig[:, t, :] * (1 - np.square(self.pstate[:, t, :])), self.Wc[:, self.out_dim:])\\\n",
    "                        + np.dot(dstate[:, t, :] * self.state[:, t - 1, :] * self.fg[:, t, :] * (1 - self.fg[:, t, :]), self.Wf[:, self.out_dim:])\\\n",
    "                        + np.dot(dstate[:, t, :] * self.pstate[:, t, :] * self.ig[:, t, :] * (1 - self.ig[:, t, :]), self.Wi[:, self.out_dim:])\\\n",
    "                        + np.dot(dO[:, t, :] * np.tanh(self.state[:, t, :]) * self.og[:, t, :] * (1 - self.og[:, t, :]), self.Wo[:, self.out_dim:])\n",
    "\n",
    "            dbo += (dO[:, t, :] * np.tanh(self.state[:, t, :]) * self.og[:, t, :] * (1 - self.og[:, t, :])).sum(axis=0)\n",
    "            dbi += (dstate[:, t, :] * self.pstate[:, t, :] * self.ig[:, t, :] * (1 - self.ig[:, t, :])).sum(axis=0)\n",
    "            dbf += (dstate[:, t, :] * self.state[:, t - 1, :] * self.fg[:, t, :] * (1 - self.fg[:, t, :])).sum(axis=0)\n",
    "            dbc += (dstate[:, t, :] * self.ig[:, t, :] * (1 - np.square(self.pstate[:, t, :]))).sum(axis=0)\n",
    "\n",
    "        dW = np.vstack([dWf, dWi, dWo, dWc]) + self.reg * self.W\n",
    "        db = np.vstack([dbf, dbi, dbo, dbc])\n",
    "        # print ('dX')\n",
    "        # print (dX)\n",
    "        # print ('dstate')\n",
    "        # print (dstate)\n",
    "        dX[mask, :] = 0\n",
    "        return (dX[:, 1:, :], dW, db)\n",
    "    \n",
    "class Embeddings(object):\n",
    "    def __init__(self, num_embeddings, embedding_dim, pad_idx=None, trainable=True, reg=0.0001):\n",
    "        self.W = None\n",
    "        self.b = 0.\n",
    "        self.reg = reg\n",
    "        self.out_dim = embedding_dim\n",
    "        self.inp_dim = num_embeddings\n",
    "        self.pad_idx = pad_idx\n",
    "        self.trainable = trainable\n",
    "        self.inp = None\n",
    "        self.next = None\n",
    "        self.momment1 = None\n",
    "        self.momment2 = None\n",
    "        if num_embeddings:\n",
    "            self.W = np.random.normal(0, 1, (self.out_dim, self.inp_dim))\n",
    "            self.momment1 = np.zeros_like(self.W)\n",
    "            self.momment2 = np.zeros_like(self.W)\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        inp.next = self\n",
    "        if self.inp_dim != inp.out_dim:\n",
    "            raise ValueError('num_embeddings do not match out_dim of inputs')\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (self.__class__.__name__ + ' embedding_dim: ' + str(self.out_dim) + ' num_embeddings: ' + str(self.inp_dim))\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        self.X = X\n",
    "        # inputs are encoded tokens with token_ids\n",
    "        # shape of X: (N, T)\n",
    "        \n",
    "        out = np.take(self.W.T, X, axis=0)\n",
    "        # output shape: N, T, emb_dim\n",
    "        if self.pad_idx is not None:\n",
    "            mask = (X != self.pad_idx).sum(axis=1)\n",
    "        return out if self.pad_idx is None else {'input_ids': out, 'seq_lens': mask}\n",
    "    \n",
    "    def backward(self, dO):\n",
    "        dW = np.zeros_like(self.W)\n",
    "        # dW[:, self.X] += dO\n",
    "        if self.trainable:\n",
    "            np.add.at(dW.T, np.s_[self.X, :], dO)\n",
    "        # dW += self.reg * self.W\n",
    "        return 0, dW, None\n",
    "\n",
    "    \n",
    "class Activation(object):\n",
    "    def __init__(self, func='relu'): # options: relu, softmax_with_cat_cross_entropy (softmax)\n",
    "        self.act_function = func\n",
    "        self.next = None\n",
    "\n",
    "    def __call__(self, inp=None):\n",
    "        self.inp = inp\n",
    "        inp.next = self\n",
    "        self.inp_dim = inp.out_dim\n",
    "        self.out_dim = self.inp_dim\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (self.__class__.__name__ + ' ' + self.act_function + ' output: ' + str(self.out_dim) + ' input: ' + str(self.inp_dim))\n",
    "    \n",
    "    def forward(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        if self.act_function == 'relu':\n",
    "            out = np.maximum(0, X)\n",
    "            activations = out\n",
    "        elif self.act_function == 'softmax':\n",
    "            exps = np.exp(X - np.max(X, axis=1, keepdims=True))\n",
    "            activations = exps / np.sum(exps, axis=1, keepdims=True)\n",
    "            activations = np.where(activations > 1.0e-10, activations, 1.0e-10)\n",
    "            out = np.mean(-1*np.sum(y * np.log(activations), axis=1))\n",
    "        elif self.act_function == 'sigmoid_with_bin_cross_entropy':\n",
    "            sig = 1/(1 + np.exp(-X))\n",
    "            activations = sig\n",
    "            activations = np.where(activations > 1.0e-10, activations, 1.0e-10)\n",
    "            out = np.mean(-1*((y * np.log(activations)) + ((1 - y) * np.log(1 - activations))))\n",
    "        elif self.act_function == 'sigmoid':\n",
    "            activations = 1/(1 + np.exp(-X))\n",
    "            activations = np.where(activations > 1.0e-7, activations, 1.0e-7)\n",
    "            activations = np.where(activations < 1 - 1.0e-7, activations, 1 - 1.0e-7)\n",
    "            out = activations\n",
    "        self.activations = activations\n",
    "        return out\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        _ = self.forward(X, y)\n",
    "        return self.activations\n",
    "    \n",
    "    def backward(self, dO=None):\n",
    "        if self.act_function == 'relu':\n",
    "            dX = np.where(self.X < 0, 0, 1) * dO\n",
    "        elif self.act_function == 'softmax' or self.act_function == 'sigmoid_with_bin_cross_entropy':\n",
    "            dX = self.activations - self.y\n",
    "        elif self.act_function == 'sigmoid':\n",
    "            dX = self.activations * (1 - self.activations) * dO\n",
    "        return (dX, None, None)\n",
    "    \n",
    "class Loss(object):\n",
    "    def __init__(self, loss_fn): # options: mse\n",
    "        self.loss_function = loss_fn\n",
    "        self.next = None\n",
    "\n",
    "    def __call__(self, inp):\n",
    "        self.inp = inp\n",
    "        inp.next = self\n",
    "        self.inp_dim = inp.out_dim\n",
    "        self.out_dim = self.inp_dim\n",
    "        return self\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return (self.__class__.__name__ + ' ' + self.loss_function + ' output: ' + str(self.out_dim) + ' input: ' + str(self.inp_dim))\n",
    "    \n",
    "    def forward(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        if self.loss_function == 'mse':\n",
    "            loss = np.mean((X - y)**2)\n",
    "        return loss\n",
    "    \n",
    "    def predict(self, X, y=None):\n",
    "        return X\n",
    "    \n",
    "    def backward(self, dO=None):\n",
    "        if self.loss_function == 'mse':\n",
    "            dX = 2*(self.X - self.y)\n",
    "        return (dX, None, None)\n",
    "\n",
    "class Optimizer(object): # SGD implementation\n",
    "    def __init__(self, lr=0.001, b1=0.9, b2=0.999):\n",
    "        self.b1, self.b2 = b1, b2\n",
    "        self.eps = 1e-8\n",
    "        self.t = 1\n",
    "        self.lr = lr\n",
    "        self.loss = []\n",
    "\n",
    "    def run_forward(self, input_layer, X, y):\n",
    "        layer = input_layer\n",
    "        out = X\n",
    "        while (layer):\n",
    "            # print (layer)\n",
    "            # print (out.shape)\n",
    "            out = layer.forward(out, y)\n",
    "            layer = layer.next\n",
    "        loss = out\n",
    "        return loss\n",
    "\n",
    "    def optimize_step(self, out_layer, verbose=False):\n",
    "        layer = out_layer \n",
    "        t = self.t\n",
    "        dO = 1  \n",
    "        lr = self.lr \n",
    "        while (layer):\n",
    "            # print (layer)\n",
    "            dO, dW, db = layer.backward(dO)\n",
    "            if dW is not None:\n",
    "                moment1 = (self.b1 * layer.momment1) + ((1 - self.b1) * dW)\n",
    "                moment2 = (self.b2 * layer.momment2) + ((1 - self.b2) * dW**2)\n",
    "                mt1_hat = moment1/(1 - self.b1**t)\n",
    "                mt2_hat = moment2/(1 - self.b2**t)\n",
    "                # W = layer.W - (lr * dW)\n",
    "                W = layer.W - (lr * mt1_hat/(np.sqrt(mt2_hat) + self.eps))\n",
    "                layer.W = W\n",
    "                layer.momment1 = moment1\n",
    "                layer.momment2 = moment2\n",
    "                if verbose >= 10:\n",
    "                    print (layer)\n",
    "                    print ('dW:', dW)\n",
    "                    print (\"updated W:\")\n",
    "                    print (W)\n",
    "\n",
    "            if db is not None:\n",
    "                moment_b1 = (self.b1 * layer.momment_b1) + ((1 - self.b1) * db)\n",
    "                moment_b2 = (self.b2 * layer.momment_b2) + ((1 - self.b2) * db**2)\n",
    "                mt1_hat = moment_b1/(1 - self.b1**t)\n",
    "                mt2_hat = moment_b2/(1 - self.b2**t)\n",
    "                # b = layer.b - (lr * db)\n",
    "                b = layer.b - (lr * mt1_hat/(np.sqrt(mt2_hat) + self.eps))\n",
    "                layer.b = b\n",
    "                layer.momment_b1 = moment_b1\n",
    "                layer.momment_b2 = moment_b2\n",
    "                if verbose >= 10:\n",
    "                    print ('db:', db)\n",
    "                    print (\"updated b:\", b)\n",
    "\n",
    "            self.t = t + 1\n",
    "            layer = layer.inp\n",
    "        return self.t\n",
    "\n",
    "    def train(self, input_layer, out_layer, \n",
    "              X, y, batch_size=None, \n",
    "              patience=20, epochs=None, \n",
    "              verbose=False, loss_tr_ep=1.0e-10,\n",
    "              inputs_batched=False):\n",
    "        patience = patience\n",
    "        loss_tracker = []\n",
    "        epoch = 0\n",
    "        if not epochs:\n",
    "            epochs = 1e10\n",
    "        if not patience:\n",
    "            patience = epochs + 1\n",
    "        patience_remaining = patience\n",
    "        \n",
    "        if not isinstance(batch_size, int) and not inputs_batched:\n",
    "            batch_size = X.shape[0]\n",
    "        elif inputs_batched:\n",
    "            batch_size = X[0].shape[0]\n",
    "        \n",
    "        num_batches = len(X) if inputs_batched else int(np.ceil(X.shape[0]/batch_size))\n",
    "        \n",
    "        print (\"Using batch_size of\", batch_size)\n",
    "        while (patience_remaining > 0 and epochs > epoch):\n",
    "            loss_tracker_epoch = []\n",
    "            for i in tqdm.tqdm(range(num_batches), disable=not verbose):\n",
    "                if inputs_batched: # X is list of precreated batched\n",
    "                    X_batch = X[i]\n",
    "                    y_batch = y[i]\n",
    "                else: # X is the entire data and batches are created here\n",
    "                    up_ind = min(X.shape[0], (i + 1) * batch_size)\n",
    "                    X_batch = X[i * batch_size: up_ind]\n",
    "                    y_batch = y[i * batch_size: up_ind]\n",
    "                loss = self.run_forward(input_layer, X_batch, y_batch)\n",
    "                timestep = self.optimize_step(out_layer, verbose=verbose)\n",
    "                loss_tracker_epoch.append(loss)\n",
    "            epoch_loss = np.mean(loss_tracker_epoch)\n",
    "            epoch += 1\n",
    "            \n",
    "            if len(loss_tracker) > 0 and epoch_loss + loss_tr_ep > min(loss_tracker):\n",
    "                patience_remaining -= 1\n",
    "            else:\n",
    "                patience_remaining = patience\n",
    "            loss_tracker.append(epoch_loss)\n",
    "\n",
    "            print ('epoch:', epoch, 'loss:', epoch_loss)\n",
    "        self.loss = self.loss + loss_tracker\n",
    "    \n",
    "    def predict(self, input_layer, X, y, batch_size=None, verbose=False):\n",
    "        out_list = []\n",
    "        if not isinstance(batch_size, int):\n",
    "            batch_size = X.shape[0]\n",
    "        print (\"Using batch_size of\", batch_size)\n",
    "        for i in range(int(np.ceil(X.shape[0]/batch_size))):\n",
    "            up_ind = min(X.shape[0], (i + 1) * batch_size)\n",
    "            X_batch = X[i * batch_size: up_ind]\n",
    "            y_batch = y[i * batch_size: up_ind]\n",
    "            layer = input_layer\n",
    "            out = X_batch\n",
    "            while (layer):\n",
    "                # print (layer)\n",
    "                # print (out.shape)\n",
    "                if isinstance(layer, (Activation, Loss)):\n",
    "                # if isinstance(layer, relu1.__class__):\n",
    "                    out = layer.predict(out, y_batch)\n",
    "                else:\n",
    "                    out = layer.forward(out, y_batch)\n",
    "                if verbose:\n",
    "                    print (layer)\n",
    "                    print (out)\n",
    "                layer = layer.next\n",
    "            out_list.append(out)\n",
    "        return np.vstack(out_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from irony import load_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, train_labels, test_sentences, test_labels, label2i = load_datasets()\n",
    "\n",
    "# TODO: Split train into train/dev\n",
    "train_sentences, dev_sentences, train_labels, dev_labels = train_test_split(train_sentences, train_labels, train_size=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing Text: 100%|██████████| 3834/3834 [00:00<00:00, 22817.51it/s]\n",
      "Vectorizing Text: 100%|██████████| 3834/3834 [00:00<00:00, 24369.04it/s]\n",
      "Vectorizing Text: 100%|██████████| 784/784 [00:00<00:00, 29609.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: Naive Bayes Classifier\n",
      "F1-score Ironic: 0.6402966625463535\n",
      "Avg F1-score: 0.6284487265300938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from irony import run_nb_baseline\n",
    "\n",
    "run_nb_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, pad_symbol: Optional[str] = \"<PAD>\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<PAD>\".\n",
    "        \"\"\"\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    def __call__(self, batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes each sentence in the batch, and pads them if necessary so\n",
    "        that we have equal length sentences in the batch.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A List of equal-length token Lists.\n",
    "        \"\"\"\n",
    "        batch = self.tokenize(batch)\n",
    "        batch = self.pad(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def tokenize(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): The input sentence.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The tokenized version of the sentence.\n",
    "        \"\"\"\n",
    "        tokenized_sents = []\n",
    "        # TODO: Tokenize the input with spacy.\n",
    "        for sent in sentences:\n",
    "            sent_tokens = [token.text.lower() for token in self.nlp(sent)]\n",
    "        # TODO: Make sure the start token is the special <SOS> token and the end token\n",
    "        #       is the special <EOS> token\n",
    "            sent_tokens = ['<SOS>'] + sent_tokens + ['<EOS>']\n",
    "            tokenized_sents.append(sent_tokens)\n",
    "\n",
    "        return tokenized_sents\n",
    "\n",
    "    def pad(self, batch: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Appends pad symbols to each tokenized sentence in the batch such that\n",
    "        every List of tokens is the same length. This means that the max length sentence\n",
    "        will not be padded.\n",
    "\n",
    "        Args:\n",
    "            batch (List[List[str]]): Batch of tokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: Batch of padded tokenized sentences. \n",
    "        \"\"\"\n",
    "        # TODO: For each sentence in the batch, append the special <P>\n",
    "        #       symbol to it n times to make all sentences equal length\n",
    "        out_batch = []\n",
    "        max_len = max([len(sent) for sent in batch])\n",
    "        for sent in batch:\n",
    "            out_batch.append(sent + [self.pad_symbol]*(max_len - len(sent)))\n",
    "        return out_batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary of the dataset: use both training and test sets here\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "all_data = train_sentences + dev_sentences + test_sentences\n",
    "my_tokenizer = Tokenizer()\n",
    "\n",
    "tokenized_data = my_tokenizer.tokenize(all_data)\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "\n",
    "with open('vocab.txt', 'w') as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = 'glove.twitter.27B.100d.txt'\n",
    "vocab_path = \"./vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "# import torch\n",
    "\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab_path: str\n",
    ") -> Tuple[Dict[str, int], np.ndarray]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Note that we have provided the entire vocab for train and test, so that for practical purposes\n",
    "    we can simply load those words in the vocab, rather than all 27B embeddings\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()]) \n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            # TODO: Build word2i and vectors such that\n",
    "            #       each word points to the index of its vector,\n",
    "            #       and only words that exist in `vocab` are in our embeddings\n",
    "            if word in vocab:\n",
    "                word2i.update({word: i})\n",
    "                vectors.append(weights)\n",
    "                i += 1\n",
    "            # raise NotImplementedError\n",
    "\n",
    "    return (word2i, np.array(vectors, dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oovs(vocab_path: str, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()])\n",
    "    \n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = vocab - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)\n",
    "\n",
    "def intialize_new_embedding_weights(num_embeddings: int, dim: int) -> np.ndarray:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    # TODO: Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    #      That is, a normal distribution with mean 0 and standard deviation of dim^-0.5\n",
    "    oov_emb = np.random.normal(0, 1/np.sqrt(dim), (num_embeddings, dim))\n",
    "    return oov_emb\n",
    "\n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: np.ndarray,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], np.ndarray]:\n",
    "    # TODO: Add the oov words to the dict, assigning a new index to each\n",
    "    max_ind = max(glove_word2i.values())\n",
    "    glove_word2i.update({oov: new_ind for oov, new_ind in zip(oovs, range(max_ind + 1, max_ind + 1 + len(oovs)))})\n",
    "    # TODO: Concatenate a new row to embeddings for each oov\n",
    "    #       initialize those new rows with `intialize_new_embedding_weights`\n",
    "    new_glove_embeddings = np.vstack((glove_embeddings, intialize_new_embedding_weights(len(oovs), glove_embeddings.shape[1])))\n",
    "    # TODO: Return the tuple of the dictionary and the new embeddings matrix\n",
    "    return glove_word2i, new_glove_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from /Users/anupbhutada/Documents/Courses/Natural Langauge Processing/Assignment4/assignment_4/glove.twitter.27B.100d.txt...\n"
     ]
    }
   ],
   "source": [
    "def make_batches(sequences: List[str], batch_size: int) -> List[List[str]]:\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    # TODO\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        yield sequences[i: i + batch_size]\n",
    "\n",
    "\n",
    "# TODO: Set your preferred batch size\n",
    "batch_size = 16\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# We make batches now and use those.\n",
    "batch_tokenized = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "for batch in make_batches(train_sentences, batch_size):\n",
    "    batch_tokenized.append(tokenizer(batch))\n",
    "\n",
    "\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embedding_path,\n",
    "    vocab_path\n",
    ")\n",
    "\n",
    "# Find the out-of-vocabularies\n",
    "oovs = get_oovs(vocab_path, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6002, 15299)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oovs), len(word2i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions to encode your batches before you call the train loop.\n",
    "\n",
    "def encode_sentences(batch: List[List[str]], word2i: Dict[str, int]) -> np.ndarray:\n",
    "    \"\"\"Encode the tokens in each sentence in the batch with a dictionary\n",
    "\n",
    "    Args:\n",
    "        batch (List[List[str]]): The padded and tokenized batch of sentences.\n",
    "        word2i (Dict[str, int]): The encoding dictionary.\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: The tensor of encoded sentences.\n",
    "    \"\"\"\n",
    "    UNK_IDX = word2i[\"<UNK>\"]\n",
    "    tensors = []\n",
    "    for sent in batch:\n",
    "        tensors.append(np.array([word2i.get(w, UNK_IDX) for w in sent], dtype='int'))\n",
    "        \n",
    "    return np.stack(tensors)\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[int]) -> np.ndarray:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    arr = np.array([int(l) for l in labels], dtype='int')\n",
    "    out = np.zeros((arr.shape[0], 2))\n",
    "    out[np.arange(arr.shape[0]), arr] = 1\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches...\n",
      "Encoding batches...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the model and run the training loop \n",
    "#       on your train/dev splits. Set and tweak hyperparameters.\n",
    "batch_size = 32\n",
    "\n",
    "print (\"Creating batches...\")\n",
    "batch_train_tokenized = []\n",
    "batch_tokenized_train = [tokenizer(batch) for batch in make_batches(train_sentences, batch_size)]\n",
    "batch_labels_train = list(make_batches(train_labels, batch_size))\n",
    "batch_tokenized_dev = [tokenizer(batch) for batch in make_batches(dev_sentences, batch_size)]\n",
    "batch_labels_dev = list(make_batches(dev_labels, batch_size))\n",
    "\n",
    "# print (\"Encoding batches...\")\n",
    "# batch_sentences_train = [encode_sentences(batch, word2i) for batch in batch_tokenized_train]\n",
    "# batch_labels_train = [encode_labels(batch_labels).reshape(-1, 1) for batch_labels in batch_labels_train]\n",
    "# batch_sentences_dev = [encode_sentences(batch, word2i) for batch in batch_tokenized_dev]\n",
    "# batch_labels_dev = [encode_labels(batch_labels).reshape(-1, 1) for batch_labels in batch_labels_dev]\n",
    "\n",
    "print (\"Encoding batches...\")\n",
    "batch_sentences_train = [encode_sentences(batch, word2i) for batch in batch_tokenized_train]\n",
    "batch_labels_train = [encode_labels(batch_labels) for batch_labels in batch_labels_train]\n",
    "batch_sentences_dev = [encode_sentences(batch, word2i) for batch in batch_tokenized_dev]\n",
    "batch_labels_dev = [encode_labels(batch_labels) for batch_labels in batch_labels_dev]\n",
    "\n",
    "print (\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "embedding_layer = Embeddings(num_embeddings=embeddings.shape[0], \n",
    "                             embedding_dim=embeddings.shape[1], \n",
    "                             pad_idx=word2i.get('<PAD>'),\n",
    "                             trainable=False)\n",
    "embedding_layer.W = embeddings.T.copy()\n",
    "lstm1 = LSTM(hidden_units=32, return_seq=True, return_mask=True, reg=0.0)(embedding_layer)\n",
    "lstm = LSTM(hidden_units=32, return_seq=False, return_mask=False, reg=0.0)(lstm1)\n",
    "dense = Dense(out_dim=2, reg=1e-5)(lstm)\n",
    "out = Activation(func='softmax')(dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(lr=0.0005)\n",
    "# opt.run_forward(embedding_layer, batch_sentences_train[0][:2], np.array([[1], [0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.6898707811905954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.6625442806437766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.623647019672039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 24.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.5989175510142293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 24.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.5775970506589221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 0.5554375102782249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 0.533283039629901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 0.5120307482097481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.4904869924634359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 24.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.46965137724356054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, batch_sentences_train, batch_labels_train, epochs=10, verbose=True, inputs_batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 24.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.4490552356553369\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.42840454854736804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.40757502460038525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.3855895878466547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.36759962774402394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 0.3516726393946023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 0.3490585615402517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 0.32920309867864705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.30185517379402316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.27749569631309684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, batch_sentences_train, batch_labels_train, epochs=10, verbose=True, inputs_batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.25759218029766573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.23643022500643596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.2320291561273965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.21417932766611947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.20630273688194267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, batch_sentences_train, batch_labels_train, epochs=5, verbose=True, inputs_batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.22514189563024053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.23502655660711175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.18471396731409073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.17175057873317187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 114/114 [00:04<00:00, 25.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.17331602316559108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, batch_sentences_train, batch_labels_train, epochs=5, verbose=True, inputs_batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.244748763374625"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.run_forward(embedding_layer, batch_sentences_dev[0], batch_labels_dev[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.    , 0.    , 1.    , 0.    ],\n",
       "       [0.9995, 0.0005, 1.    , 0.    ],\n",
       "       [0.0003, 0.9997, 0.    , 1.    ],\n",
       "       [0.0031, 0.9969, 1.    , 0.    ],\n",
       "       [0.9986, 0.0014, 1.    , 0.    ],\n",
       "       [0.9934, 0.0066, 1.    , 0.    ],\n",
       "       [0.0006, 0.9994, 0.    , 1.    ],\n",
       "       [0.9774, 0.0226, 0.    , 1.    ],\n",
       "       [0.0029, 0.9971, 1.    , 0.    ],\n",
       "       [0.0001, 0.9999, 0.    , 1.    ],\n",
       "       [0.0068, 0.9932, 0.    , 1.    ],\n",
       "       [0.0377, 0.9623, 0.    , 1.    ],\n",
       "       [0.998 , 0.002 , 1.    , 0.    ],\n",
       "       [0.9007, 0.0993, 0.    , 1.    ],\n",
       "       [0.9976, 0.0024, 1.    , 0.    ],\n",
       "       [0.0006, 0.9994, 0.    , 1.    ],\n",
       "       [0.0031, 0.9969, 1.    , 0.    ],\n",
       "       [0.0017, 0.9983, 0.    , 1.    ],\n",
       "       [0.0051, 0.9949, 0.    , 1.    ],\n",
       "       [1.    , 0.    , 0.    , 1.    ],\n",
       "       [0.9068, 0.0932, 0.    , 1.    ],\n",
       "       [0.9511, 0.0489, 1.    , 0.    ],\n",
       "       [0.0001, 0.9999, 0.    , 1.    ],\n",
       "       [0.6782, 0.3218, 1.    , 0.    ],\n",
       "       [0.9401, 0.0599, 1.    , 0.    ],\n",
       "       [0.086 , 0.914 , 1.    , 0.    ],\n",
       "       [0.0037, 0.9963, 1.    , 0.    ],\n",
       "       [0.9995, 0.0005, 0.    , 1.    ],\n",
       "       [0.7337, 0.2663, 0.    , 1.    ],\n",
       "       [0.0599, 0.9401, 1.    , 0.    ],\n",
       "       [0.9999, 0.0001, 1.    , 0.    ],\n",
       "       [0.9994, 0.0006, 1.    , 0.    ]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)\n",
    "np.hstack([out.activations, batch_labels_dev[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((opt.predict(embedding_layer, batch_sentences_dev[1], batch_labels_dev[1]) > 0.5).astype('int') == batch_labels_dev[1]).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import f1_score\n",
    "\n",
    "def predict_batches(input_layer, batched_X, batched_y):\n",
    "    all_preds = []\n",
    "    for X, y in zip(batched_X, batched_y):\n",
    "        preds = opt.predict(input_layer, X, y)\n",
    "        all_preds.append(preds)\n",
    "    return np.vstack(all_preds)\n",
    "\n",
    "def get_pred_and_true_labels(true_y=None, pred_y=None):\n",
    "    if true_y is not None:\n",
    "        true_y = np.argmax(true_y, axis=1)\n",
    "    if pred_y is not None:\n",
    "        pred_y = np.argmax(pred_y, axis=1)\n",
    "    return true_y, pred_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9349258649093904"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_batches(embedding_layer, batch_sentences_train, batch_labels_train)\n",
    "y_true = np.vstack(batch_labels_train)\n",
    "((preds > 0.5).astype('float') == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6458333333333334"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = predict_batches(embedding_layer, batch_sentences_dev, batch_labels_dev)\n",
    "y_true = np.vstack(batch_labels_dev)\n",
    "((preds > 0.5).astype('float') == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6179775280898876"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Validation F1-score\n",
    "f1_score(*get_pred_and_true_labels(predict_batches(embedding_layer, batch_sentences_dev, batch_labels_dev), np.vstack(batch_labels_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encode_sentences(tokenizer(test_sentences), word2i)\n",
    "test_labels_encoded = encode_labels(test_labels)\n",
    "test_labels_encoded = test_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6492346938775511"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((opt.predict(embedding_layer, test_encoded, test_labels_encoded) > 0.5).astype('int') == test_labels_encoded).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5424292845257903"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test F1-score\n",
    "true_y, pred_y = get_pred_and_true_labels(test_labels_encoded, opt.predict(embedding_layer, test_encoded, test_labels_encoded))\n",
    "f1_score(pred_y, true_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuters_data = keras.datasets.reuters\n",
    "(X_train, y_train),(X_test, y_test) = reuters_data.load_data(num_words=25000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "313.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = [len(x) for x in X_train]\n",
    "np.quantile(np.array(seq_lengths), 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pad_sequences(X_train, maxlen=350, padding='post')\n",
    "X_test = pad_sequences(X_test, maxlen=350, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8482, 350), (8482,), (500, 350), (500,), (2246, 350), (2246,))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(InputLayer(input_shape=(350,)))\n",
    "# model.add(Embedding(input_dim=25000, output_dim=64, mask_zero=True))\n",
    "# model.add(LSTM(128, return_sequences=False, dropout=0.2, recurrent_dropout=0.2))\n",
    "# model.add(Dense(64, activation='relu'))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Dense(46, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create model\n",
    "embedding_layer = Embeddings(num_embeddings=25000, \n",
    "                             embedding_dim=64, \n",
    "                             pad_idx=0,\n",
    "                             trainable=True)\n",
    "# embedding_layer.W = embeddings.T.copy()\n",
    "# lstm1 = LSTM(hidden_units=128, return_seq=True, return_mask=True, reg=0.0)(embedding_layer)\n",
    "lstm = LSTM(hidden_units=128, return_seq=False, return_mask=False, reg=1e-4)(embedding_layer)\n",
    "dense1 = Dense(out_dim=64, reg=5e-4)(lstm)\n",
    "act1 = Activation(func='relu')(dense1)\n",
    "dense = Dense(out_dim=46, reg=1e-4)(act1)\n",
    "out = Activation(func='softmax')(dense)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimizer(lr=5e-3)\n",
    "# opt.run_forward(embedding_layer, batch_sentences_train[0][:2], np.array([[1], [0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:36<00:00,  2.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 2.255361786979062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:24<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 2.1051410344001305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:34<00:00,  2.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 1.904604715212081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:35<00:00,  2.32s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 1.7064607626719517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:27<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 1.6231017069796987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:21<00:00,  2.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 1.458767529718493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:28<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 1.2633129531069447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:29<00:00,  2.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 1.089423604429264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:44<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.9511709621117089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:26<00:00,  2.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.9011357115154357\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, X_train, to_categorical(y_train), epochs=10, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:31<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.7094624225582709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:32<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.5980168502808894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:33<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.5225998740373119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:33<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.4628947940258892\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:33<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.42024021920055826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:32<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 loss: 0.3684495619195603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:33<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 loss: 0.33526827036573603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:33<00:00,  1.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 loss: 0.30713217249119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:31<00:00,  1.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 loss: 0.27593803259504734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 133/133 [02:31<00:00,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 loss: 0.2622258582089536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, X_train, to_categorical(y_train), epochs=10, batch_size=64, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:24<00:00,  2.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.21784990162273726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:14<00:00,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.17604003727031522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:16<00:00,  2.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.16121125842849707\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:14<00:00,  2.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 loss: 0.1520056982995575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:13<00:00,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 loss: 0.1446977032185888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, X_train, to_categorical(y_train), epochs=5, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr = 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:21<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 0.1174973161716144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:18<00:00,  2.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 loss: 0.11639521939871779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67/67 [02:18<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 loss: 0.11538785745599028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, X_train, to_categorical(y_train), epochs=3, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.lr = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer.momment1 *= 0\n",
    "embedding_layer.momment2 *= 0\n",
    "lstm.momment1 *= 0\n",
    "lstm.momment2 *= 0\n",
    "dense1.momment1 *= 0\n",
    "dense1.momment2 *= 0\n",
    "dense.momment1 *= 0\n",
    "dense.momment2 *= 0\n",
    "opt.t = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/67 [00:00<?, ?it/s]/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:375: RuntimeWarning: divide by zero encountered in divide\n",
      "  mt1_hat = moment1/(1 - self.b1**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:376: RuntimeWarning: divide by zero encountered in divide\n",
      "  mt2_hat = moment2/(1 - self.b2**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:378: RuntimeWarning: invalid value encountered in divide\n",
      "  W = layer.W - (lr * mt1_hat/(np.sqrt(mt2_hat) + self.eps))\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:391: RuntimeWarning: divide by zero encountered in divide\n",
      "  mt1_hat = moment_b1/(1 - self.b1**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:392: RuntimeWarning: divide by zero encountered in divide\n",
      "  mt2_hat = moment_b2/(1 - self.b2**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:394: RuntimeWarning: invalid value encountered in divide\n",
      "  b = layer.b - (lr * mt1_hat/(np.sqrt(mt2_hat) + self.eps))\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:391: RuntimeWarning: invalid value encountered in divide\n",
      "  mt1_hat = moment_b1/(1 - self.b1**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:392: RuntimeWarning: invalid value encountered in divide\n",
      "  mt2_hat = moment_b2/(1 - self.b2**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:375: RuntimeWarning: invalid value encountered in divide\n",
      "  mt1_hat = moment1/(1 - self.b1**t)\n",
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_70986/4264927426.py:376: RuntimeWarning: invalid value encountered in divide\n",
      "  mt2_hat = moment2/(1 - self.b2**t)\n",
      "100%|██████████| 67/67 [02:25<00:00,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 loss: 22.695382931101676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "opt.train(embedding_layer, out, X_train, to_categorical(y_train), epochs=1, batch_size=128, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6807436519317873"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.run_forward(embedding_layer, X_val, to_categorical(y_val, 46))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.726"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = predict_batches(embedding_layer, batch_sentences_train, batch_labels_train)\n",
    "preds = opt.predict(embedding_layer, X_val, to_categorical(y_val, 46))\n",
    "# y_true = np.vstack(batch_labels_train)\n",
    "pred_y = np.argmax(preds, axis=1)\n",
    "(pred_y == y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 2246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.711487088156723"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = predict_batches(embedding_layer, batch_sentences_train, batch_labels_train)\n",
    "preds = opt.predict(embedding_layer, X_test, to_categorical(y_test))\n",
    "# y_true = np.vstack(batch_labels_train)\n",
    "pred_y = np.argmax(preds, axis=1)\n",
    "(pred_y == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.27272727, 0.61904762, 0.47368421, 0.91183575, 0.80887012,\n",
       "       0.        , 0.57142857, 0.33333333, 0.48837209, 0.72      ,\n",
       "       0.625     , 0.58441558, 0.13333333, 0.28947368, 0.        ,\n",
       "       0.        , 0.60550459, 0.08695652, 0.58536585, 0.61481481,\n",
       "       0.40944882, 0.48888889, 0.        , 0.16666667, 0.28571429,\n",
       "       0.5483871 , 0.15384615, 0.4       , 0.1       , 0.25      ,\n",
       "       0.31578947, 0.15384615, 0.38095238, 0.6       , 0.53333333,\n",
       "       0.4       , 0.28571429, 0.        , 0.        , 0.16666667,\n",
       "       0.28571429, 0.10526316, 0.        , 0.5       , 0.88888889,\n",
       "       0.5       ])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_y, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36181051917055607"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_y, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_y = np.argmax(preds, axis=1)\n",
    "# (pred_y == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.73"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = predict_batches(embedding_layer, batch_sentences_train, batch_labels_train)\n",
    "preds = opt.predict(embedding_layer, X_val, to_categorical(y_val, 46))\n",
    "# y_true = np.vstack(batch_labels_train)\n",
    "pred_y = np.argmax(preds, axis=1)\n",
    "(pred_y == y_val).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 2246\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.711487088156723"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preds = predict_batches(embedding_layer, batch_sentences_train, batch_labels_train)\n",
    "preds = opt.predict(embedding_layer, X_test, to_categorical(y_test))\n",
    "# y_true = np.vstack(batch_labels_train)\n",
    "pred_y = np.argmax(preds, axis=1)\n",
    "(pred_y == y_test).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.26086957, 0.61111111, 0.47368421, 0.91206792, 0.80541103,\n",
       "       0.        , 0.57142857, 0.33333333, 0.49411765, 0.70833333,\n",
       "       0.59701493, 0.57324841, 0.13333333, 0.27848101, 0.        ,\n",
       "       0.        , 0.60550459, 0.1       , 0.6       , 0.62222222,\n",
       "       0.40625   , 0.46511628, 0.        , 0.18181818, 0.26666667,\n",
       "       0.56666667, 0.16666667, 0.4       , 0.11764706, 0.25      ,\n",
       "       0.31578947, 0.16666667, 0.4       , 0.44444444, 0.53333333,\n",
       "       0.4       , 0.25      , 0.        , 0.        , 0.18181818,\n",
       "       0.28571429, 0.11764706, 0.        , 0.5       , 0.88888889,\n",
       "       0.5       ])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_y, average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3583759796436626"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(y_test, pred_y, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = predict_batches(embedding_layer, batch_sentences_dev, batch_labels_dev)\n",
    "y_true = np.vstack(batch_labels_dev)\n",
    "((preds > 0.5).astype('float') == y_true).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n",
      "Using batch_size of 32\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7522935779816514"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Validation F1-score\n",
    "f1_score(*get_pred_and_true_labels(predict_batches(embedding_layer, batch_sentences_dev, batch_labels_dev), np.vstack(batch_labels_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encode_sentences(tokenizer(test_sentences), word2i)\n",
    "test_labels_encoded = encode_labels(test_labels)\n",
    "test_labels_encoded = test_labels_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6594387755102041"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "((opt.predict(embedding_layer, test_encoded, test_labels_encoded) > 0.5).astype('int') == test_labels_encoded).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using batch_size of 784\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6377204884667571"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test F1-score\n",
    "true_y, pred_y = get_pred_and_true_labels(test_labels_encoded, opt.predict(embedding_layer, test_encoded, test_labels_encoded))\n",
    "f1_score(pred_y, true_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 788,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Sweet',\n",
       " 'United',\n",
       " 'Nations',\n",
       " 'video',\n",
       " '.',\n",
       " 'Just',\n",
       " 'in',\n",
       " 'time',\n",
       " 'for',\n",
       " 'Christmas',\n",
       " '.',\n",
       " '#',\n",
       " 'imagine',\n",
       " '#',\n",
       " 'NoReligion',\n",
       " ' ',\n",
       " 'http://t.co/fej2v3OUBR']"
      ]
     },
     "execution_count": 788,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[t.text for t in tokenizer.nlp(\"Sweet United Nations video. Just in time for Christmas. #imagine #NoReligion  http://t.co/fej2v3OUBR\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 790,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'@restlessduncan',\n",
       " 'women|#misogyny|',\n",
       " 'grinning_face_with_big_eyes::thumbs_up',\n",
       " '@grimmers',\n",
       " '@mtnza',\n",
       " 'loveeee',\n",
       " 'web|please',\n",
       " '@twitwhizz',\n",
       " 'guardiansofpeace',\n",
       " 'http://t.co/3io7qlk0lr',\n",
       " 'face_with_tears_of_joy::face_with_tears_of_joy::face_with_tears_of_joy::face_with_tears_of_joy',\n",
       " 'http://t.co/9wjfiq6wmk',\n",
       " '@siddharth_0703',\n",
       " 'http://t.co/gmfru7gmir',\n",
       " '90ssarcasm',\n",
       " 'http://t.co/yygdzirwfz',\n",
       " '@quiksilverindia',\n",
       " 'nigher',\n",
       " 'morefollowersplease',\n",
       " '@lariatofhestia',\n",
       " 'http://t.co/3ov57zhqch',\n",
       " 'idc#istillloveyou',\n",
       " 'http://t.co/qulijvfndk',\n",
       " '||#no',\n",
       " 'http://t.co/delfxn0bpi',\n",
       " '@virginmedia',\n",
       " 'http://t.co/hvxmxrdktm',\n",
       " '@oneplanetmikey',\n",
       " 'nmucomputersarethebest',\n",
       " '2012',\n",
       " '@progressmich',\n",
       " '@thelexzane',\n",
       " '@worlddiamark',\n",
       " 'qhycb5zkev',\n",
       " 'orevsosu',\n",
       " '|keeping',\n",
       " 'lakhvi',\n",
       " 'loudly_crying_face::crying_face',\n",
       " '11',\n",
       " 'cancup',\n",
       " 'film2014',\n",
       " '@garnetngold22',\n",
       " '@gazthegooner84',\n",
       " 'not@allpeaceful',\n",
       " 'nic_kill',\n",
       " 'weary_face',\n",
       " '@erin_also',\n",
       " 'triedandtrue',\n",
       " 'there.also',\n",
       " 'http://t.co/aclm33f8he',\n",
       " '@andrewdbailey',\n",
       " 'fustration.|#funfact',\n",
       " '@barackobama',\n",
       " '@aftergrogblog',\n",
       " 'orionlaunch',\n",
       " ':(',\n",
       " 'truebro',\n",
       " '@kdunham4prez',\n",
       " '-3',\n",
       " 'maaayuun!!:face_with_tears_of_joy::face_with_tears_of_joy::face_with_tears_of_joy',\n",
       " '@ed_miliband',\n",
       " 'http://t.co/rgydbek0h2',\n",
       " '@braves',\n",
       " '@fireinthelyons',\n",
       " 'http://t.co/jq9ajbisbw',\n",
       " 'http://t.co/flrkr3h6kl',\n",
       " '19',\n",
       " 'likeee',\n",
       " '@countingcrowsto',\n",
       " '.@shawnatova',\n",
       " 'nosleepdecember',\n",
       " '75',\n",
       " 'http://t.co/qeupepji6',\n",
       " 'http://t.co/epygva9pwu',\n",
       " '@notthecubsway',\n",
       " 'http://t.co/otbewggbtj',\n",
       " 'oncoming_police_car',\n",
       " 'http://t.co/cy47ylctjs',\n",
       " 'pizzawins',\n",
       " '@bavarianshady',\n",
       " '@abpt_rocket',\n",
       " 'post!||#iphone',\n",
       " '7p',\n",
       " '@normanwalshuk',\n",
       " 'money|#scowls',\n",
       " 'http://t.co/mn8psbzpfh||#workfromhome',\n",
       " 'http://t.co/mvsikofvbl',\n",
       " '@the3rdeyebrand',\n",
       " '@heathmusic',\n",
       " '@whipappeal_6',\n",
       " 'thingsmichelleobamathinksareracist',\n",
       " 'winking_face_with_tongue',\n",
       " '@yn0htna',\n",
       " '@ncgunoholic',\n",
       " 'drhandsomedennis',\n",
       " 'stoleem',\n",
       " 'rhoa!!!!#love',\n",
       " 'holidayperiod',\n",
       " '@hollymanthei',\n",
       " 'out!|',\n",
       " '@courteneycox',\n",
       " 'suhs',\n",
       " '@lnritter',\n",
       " 'editorspick',\n",
       " '@laurenbailes',\n",
       " '@frankieballard',\n",
       " '@lilyallen',\n",
       " '@ibgomez',\n",
       " 'http://t.co/fxukcp3j2y',\n",
       " 'g5jly9ikqn',\n",
       " 'heavyamountsofsarcasm',\n",
       " '@peteregan6',\n",
       " '@deray',\n",
       " '@tomriordan',\n",
       " 'http://t.co/3mh7crrzn0',\n",
       " 'q2wb7riavk',\n",
       " 'trustthesystem',\n",
       " '@kels_efollin',\n",
       " 'jusygirlythings',\n",
       " ':check_mark_button',\n",
       " '@andrewannun',\n",
       " '3daughters#lucky',\n",
       " 'http://t.co/4y93g5eol3',\n",
       " 'http://t.co/01ziuh8mye',\n",
       " '@reedwilkerson',\n",
       " 'bigclub',\n",
       " 'moveitalong',\n",
       " 'http://t.co/hnwyc5jp13',\n",
       " 'theinterview',\n",
       " 'imgoingtostruggle',\n",
       " '@parisbreedenw',\n",
       " '@jk_rowling',\n",
       " '@laboureoin',\n",
       " 'http://t.co/uysi2qga4s',\n",
       " 'evil_monkey::see',\n",
       " '@nostromonavis',\n",
       " '@mickymantell',\n",
       " ';-)',\n",
       " 'lobbyinglife',\n",
       " 'fridaiis',\n",
       " '@torontostar',\n",
       " '1st',\n",
       " '-news',\n",
       " 'http://t.co/x54n9zp12',\n",
       " 'http://t.co/vmchi7xb88',\n",
       " '@profootballtalk',\n",
       " 'summer2k14',\n",
       " 'country#pdp',\n",
       " '@countingcrows',\n",
       " 'santa_claus',\n",
       " '@orthorize',\n",
       " 'price:$65.00',\n",
       " '@kradlum',\n",
       " '@crizzarce',\n",
       " 'tylerfarr',\n",
       " 'face_blowing_a_kiss',\n",
       " '@proclamationmts',\n",
       " '123',\n",
       " '@_barryisland',\n",
       " 'http://t.co/qqqci3faxa',\n",
       " '@kayleighjr',\n",
       " 'it!:smiling_face_with_heart',\n",
       " 'http://t.co/9bdkxt9gfj',\n",
       " '45',\n",
       " 'freebiebookdownload',\n",
       " 'http://t.co/cge2vlixty',\n",
       " 'jason-2',\n",
       " 'http://t.co/3kany6qhkr',\n",
       " 'mikebrown',\n",
       " 'face_with_tears_of_joy::face_with_tears_of_joy',\n",
       " '@auspublica',\n",
       " 'face_with_tears_of_joy',\n",
       " 'collegeiskillingme',\n",
       " '@gcash14',\n",
       " 'http://t.co/9yabombf6c',\n",
       " 'evc14',\n",
       " '|4',\n",
       " 'clapping_hands::clapping_hands::clapping_hands',\n",
       " '<PAD>',\n",
       " 'http://t.co/o8wr5zo2lg',\n",
       " '@mark3labs',\n",
       " 'myperiod',\n",
       " 'vmcorrupt',\n",
       " 'loudly_crying_face::loudly_crying_face',\n",
       " 'books::open_book',\n",
       " 'going?????|better',\n",
       " '@him_88',\n",
       " 'dipmagazine',\n",
       " \"bomb:'r\",\n",
       " '<UNK>',\n",
       " '@thebravesaddict',\n",
       " 'http://t.co/cwaaywuwgl',\n",
       " 'sarcasm|illinois',\n",
       " 'http://t.co/lbb7vzv6yx',\n",
       " \"~i'm\",\n",
       " 'londoncalling',\n",
       " '1989',\n",
       " 'wbbm',\n",
       " '@lancemedow',\n",
       " '6pm',\n",
       " 'http://t.co/bdsbz93pcc',\n",
       " '@pff_pete',\n",
       " '@brianbullinger',\n",
       " '@boeing',\n",
       " 'http://t.co/sdrby5tnd5',\n",
       " '@aiiamericangiri',\n",
       " 'incaseyoucanttell',\n",
       " '@travisrodgers',\n",
       " 'grinning_face::water_pistol',\n",
       " '@blissfulbrann',\n",
       " 'rt2gain',\n",
       " '@beckie0',\n",
       " 'head|#not',\n",
       " '@cecilerichards',\n",
       " 'http://t.co/njyk9hf6pq',\n",
       " '@springthunder',\n",
       " '@translink_ni',\n",
       " '@paulkrugmanblog',\n",
       " 'convos|and',\n",
       " 'https://t.co/q3oaw29ogv',\n",
       " '|.|#irony',\n",
       " 'http://t.co/mywivy4juv',\n",
       " '@nobamamustgo',\n",
       " 'ambulanced',\n",
       " 'fun#work#not#friends#only#dance#drinks#antennagrouprocks',\n",
       " 'liabeeeeee',\n",
       " '@hltvorg',\n",
       " \"they're\",\n",
       " 'unamused_face::see',\n",
       " 'psychhumor',\n",
       " '@davidwhite',\n",
       " '@mcfc',\n",
       " '@cnn9ja',\n",
       " '@kimoen_peter',\n",
       " 'http://t.co/emuwok00tl',\n",
       " 'captainpc@me.com',\n",
       " '4',\n",
       " '@vincentlaforet',\n",
       " '@neezmhaneez',\n",
       " 'middlechildsyndrome',\n",
       " 'fatbro',\n",
       " '@washwizards',\n",
       " '@taylorfisherrr',\n",
       " 'lovethwayyoulie',\n",
       " '@mike_coogan',\n",
       " 'check-',\n",
       " 'http://t.co/nysbehzjqs',\n",
       " '@bglassify',\n",
       " 'peshawarattack',\n",
       " 'blót',\n",
       " '@jumbopaperclips',\n",
       " 'http://t.co/rjtcamq9gz',\n",
       " 'pw)|humphrey',\n",
       " 'http://t.co/clfufz3vdf',\n",
       " 'nlprimetime',\n",
       " 'http://t.co/jrqic9qymk',\n",
       " 'http://t.co/alcqqsassy',\n",
       " 'long#along#time#times#no#see#me#you#i#your#say#hi#hello#want#wanna#to#meet',\n",
       " '@aliceoflalaland',\n",
       " 'http://t.co/cbs3yfykgw',\n",
       " '26/11',\n",
       " '@adamjamesberack',\n",
       " 'http://t.co/sxroxlvspp',\n",
       " '.@chrisbrett2',\n",
       " 'gardenofeden',\n",
       " '@udegmx|<3;-}><3',\n",
       " 'moneytho',\n",
       " '@cheenazafar',\n",
       " '@troyrenck',\n",
       " 'ostentatiously',\n",
       " '@coreybking',\n",
       " 'day.:unamused_face',\n",
       " '2014',\n",
       " '@ollielocke',\n",
       " 'http://t.co/csw0rfhgzz',\n",
       " 'http://t.co/yij6tihmsm',\n",
       " '-$',\n",
       " 'http://t.co/ofsmwjt646',\n",
       " 'sermonising',\n",
       " 'paycoin',\n",
       " '49',\n",
       " 'http://t.co/sne5a0eeah',\n",
       " 'http://t.co/is904hbqnk',\n",
       " '.@thatneilguy',\n",
       " 'you|http://t.co',\n",
       " '@naaaauds',\n",
       " 'earicpatten',\n",
       " 'http://t.co/0ck2vnqikr',\n",
       " '1/2',\n",
       " '@cameron_gray',\n",
       " 'flzjingleball',\n",
       " '@opedmarketing',\n",
       " '@ladysandersfarm',\n",
       " '@burninggoats',\n",
       " '65th',\n",
       " '@brownbearmike1',\n",
       " 'regressives',\n",
       " 'astonished_face::face_with_tears_of_joy::face_with_tears_of_joy',\n",
       " '21st',\n",
       " 'http://t.co/mjxdzczdud',\n",
       " '@nettaaaaaaaa',\n",
       " '77',\n",
       " '@po_st',\n",
       " '@antoineraps',\n",
       " 'nothelpfulatall',\n",
       " 'intodeadthings',\n",
       " 'winking_face',\n",
       " '@gregorroberts0n',\n",
       " 'itstruetho',\n",
       " 'red_question_mark::red_question_mark',\n",
       " 'http://t.co/unhbb7v1fv',\n",
       " 'http://t.co/h4xoh4jpwc',\n",
       " 'asgill',\n",
       " 'astonished_face::grimacing_face',\n",
       " '@hotnostrilsrfun',\n",
       " '@trags',\n",
       " '..',\n",
       " 'whatarethelivingrockpeoplefor',\n",
       " 'http://t.co/9ujqrmskjm',\n",
       " 'wheels.|',\n",
       " 'sillyvine',\n",
       " 'ihopeigetitback',\n",
       " 'purple_heart',\n",
       " 'notjustice',\n",
       " '@real_smart_guy3',\n",
       " 'http://t.co/6e4lh0wjjj',\n",
       " 'copycatsays',\n",
       " '@biggrooverecord',\n",
       " 'http://t.co/vywhaogbz9',\n",
       " 'http://t.co/qdescaifsk',\n",
       " 'http://t.co/ccfhdt73wh',\n",
       " 'ohanya',\n",
       " '12:00am',\n",
       " '12daysofchristmassweaterscontest',\n",
       " 'http://t.co/ykf7vb53rm',\n",
       " 'face?|#ursulaonamonday|#wow',\n",
       " 'tired_face::tired_face::tired_face',\n",
       " 'http://t.co/ey6n4ses9',\n",
       " '@earthpix',\n",
       " '@emthedivine',\n",
       " '@tmj4steve',\n",
       " '@yasminaben1',\n",
       " '127',\n",
       " 'day3',\n",
       " '@kwapt',\n",
       " '@thepdempsey',\n",
       " '@theoliverstone',\n",
       " 'ok_hand',\n",
       " 'cloud:|#decemberphotochallenge',\n",
       " '@ralufc',\n",
       " 'thankyoumicrosoft',\n",
       " 'euvat',\n",
       " 'http://t.co/he3ntbhtpq',\n",
       " 'christmas_tree',\n",
       " 'nocollegeneeded',\n",
       " '@nicky0472',\n",
       " 'ellielippitt',\n",
       " '@itvdean',\n",
       " '@cuidadodesalud',\n",
       " 'tvandsleep',\n",
       " 'grinning_squinting_face',\n",
       " 'denominated',\n",
       " 'vaginally',\n",
       " '1',\n",
       " '@trevordickerson',\n",
       " '@sony',\n",
       " '@andygreencre8iv',\n",
       " '@roughhousesgw',\n",
       " 'purple_heart::two_hearts',\n",
       " 'cosby|#irony',\n",
       " 'http://t.co/ghsm9zy7kw',\n",
       " '@heelteacher',\n",
       " '@lr_lam',\n",
       " 'nobelpeaceprize',\n",
       " '@numbdave',\n",
       " 'http://t.co/amrz6zvqtp',\n",
       " '@zacaparum',\n",
       " 'http://t.co/gcw05uriit',\n",
       " 'bhreketmka',\n",
       " '@hhslatino',\n",
       " '@theclash',\n",
       " '@gopittfootball',\n",
       " 'welcometotheworld',\n",
       " 'worstsongever',\n",
       " 'ohip',\n",
       " 'http://t.co/9di1xjv9uk',\n",
       " '@jus1314tin',\n",
       " '@tstg11',\n",
       " 'wheredobrokenheartsgo',\n",
       " '@metrolinx',\n",
       " 'http://t.co/ikgz8u9fos',\n",
       " 'sofulfilled',\n",
       " 'lashwkaywiduwb',\n",
       " '@ozchrisrock',\n",
       " '18th',\n",
       " '@lisawhybourn',\n",
       " 'ebtbitches',\n",
       " 'http://t.co/gysnxsrwse',\n",
       " 'http://t.co/1koldjisq3',\n",
       " '@al_muwa7id',\n",
       " '@yodelonline',\n",
       " 'analtryouts',\n",
       " 'http://t.co/rxevfbshsh',\n",
       " 'internetmarketnewbie',\n",
       " 'accie',\n",
       " 'artsy#homealone#vienna#studentlife',\n",
       " '50thfloor',\n",
       " 'http://t.co/tvkhumwagz',\n",
       " 'admire.|winston',\n",
       " 'italianbees',\n",
       " '@kennisibueno',\n",
       " '@andyburnhammp',\n",
       " '@sevangeline',\n",
       " 'http://t.co/wu41rvvvjw',\n",
       " 'squadgoals',\n",
       " '@jsteigleman',\n",
       " '@orion99da',\n",
       " '@losethosepoundz',\n",
       " 'http://t.co/13fb4tnei4',\n",
       " '@foxnewspolitics',\n",
       " '@lipsbycarla',\n",
       " 'undergroundhiphop',\n",
       " '||#ｆｏｌｌｏｗ',\n",
       " '@txrebel05',\n",
       " '1vote',\n",
       " '@gazzadowns',\n",
       " 'tuques',\n",
       " '@hausofse7en',\n",
       " 'chanelboybag',\n",
       " '@scottishlabour',\n",
       " 'smiling_face',\n",
       " 'http://t.co/n7ygekpr4l',\n",
       " '||#truestoryfam',\n",
       " 'mason,',\n",
       " '80',\n",
       " '@twitter',\n",
       " 'http://t.co/6nrijjjwmc',\n",
       " '@sportstalksc',\n",
       " '@neilby70',\n",
       " 'http://t.co/mpsxn3djqu',\n",
       " '@outtaherrrrreee',\n",
       " 'http://t.co/hj6tebh4o6',\n",
       " '@sscottamatthews',\n",
       " '~source',\n",
       " '@bophiesurch',\n",
       " 'http://t.co/ssyonx0iao',\n",
       " 'rubbingitin',\n",
       " '@lcooney',\n",
       " 'http://t.co/cjzsivqswx',\n",
       " '|what',\n",
       " '971.2',\n",
       " '@orionslu',\n",
       " '@1_free_man',\n",
       " 'actuallysorry',\n",
       " '@bbgalad#he',\n",
       " '@benpobjie',\n",
       " '@margoandhow',\n",
       " '@stevepulcinella',\n",
       " 'middlemore',\n",
       " '@d_shariatmadari',\n",
       " '@ranjenim',\n",
       " 'annoying#stoptalking',\n",
       " '@vikramchandra',\n",
       " 'adelaideroutecancelled',\n",
       " '@tomwookieford',\n",
       " 'howthegrinchstolechristmas',\n",
       " 'red_heart:#bahhumbug',\n",
       " '3:30',\n",
       " 'http://t.co/ul8uegde9g||it',\n",
       " 'http://t.co/cn4nv8fdf0',\n",
       " '@salon',\n",
       " '2000',\n",
       " '@redtaurus1',\n",
       " 'f*ck',\n",
       " '@nelza82',\n",
       " 'stormageddon',\n",
       " '@masongoodman',\n",
       " 'reeeaaalll',\n",
       " '4.0',\n",
       " 'decor|please',\n",
       " 'two_hearts::purple_heart',\n",
       " '14',\n",
       " '@planb',\n",
       " '@uklabour',\n",
       " 'bluetick',\n",
       " 'http://t.co/dusmz0kts4',\n",
       " 'emopostahead',\n",
       " 'http://t.co/qsmof8f0x4',\n",
       " 'http://t.co/5wa92n1pps',\n",
       " '@neiltyson',\n",
       " 'tropical_drink::tropical_drink::tropical_drink::tropical_drink::tropical_drink::tropical_drink',\n",
       " '@onemorejoke',\n",
       " 'haha||#repost',\n",
       " 'http://t.co/6bfvvn4qz8',\n",
       " '||#follow',\n",
       " '@hallieciera',\n",
       " '@doodlebug0',\n",
       " '@pattistanger',\n",
       " 'http://t.co/fepzhfy79z',\n",
       " '||',\n",
       " 'http://t.co/ncat8xxqxq',\n",
       " 'went|but',\n",
       " '5k',\n",
       " 'flushed_face::smirking_face',\n",
       " 'will|work',\n",
       " 'http://t.co/1bfosiwni5',\n",
       " '@chels_2325',\n",
       " '913',\n",
       " '@mericamcfreedom',\n",
       " '@imbalaska',\n",
       " '@fleureast',\n",
       " '@willbmx',\n",
       " 'http://t.co/e189ihbpzr',\n",
       " 'bhpbilliton',\n",
       " 'lavashak',\n",
       " '@giantfootyguy',\n",
       " 'weisser',\n",
       " '@harrythetech76',\n",
       " 'fakefan',\n",
       " 'essentialservice',\n",
       " '39.99',\n",
       " 'mariland',\n",
       " '@valb00',\n",
       " 'crying_face::crying_face',\n",
       " 'yclnationalcongress',\n",
       " '@iwritethings23',\n",
       " '@espngreeny',\n",
       " 'person_tipping_hand::christmas_tree::santa_claus',\n",
       " '@peppa5',\n",
       " '@rosiemorse',\n",
       " 's3',\n",
       " '-7',\n",
       " 'http://t.co/qebtdsvort',\n",
       " 'reengaged',\n",
       " 'pantsdowntiggy',\n",
       " 'wage|#sad',\n",
       " 'http://t.co/3eamnvete1',\n",
       " 'nuds',\n",
       " 'http://t.co/zca7uxxhrh',\n",
       " 'penedesfera',\n",
       " 'http://t.co/zevvcvjrr5',\n",
       " 'killsssss',\n",
       " 'karmicthought',\n",
       " '@delbartonhockey',\n",
       " 'http://t.co/t4r61yqf6',\n",
       " 'http://t.co/n9b7t2xj84',\n",
       " 'irish!!of',\n",
       " '@cougartownrally',\n",
       " 'xringmiracle',\n",
       " '@sanfranciscovc',\n",
       " '@bellsbrewery',\n",
       " 'faaaaa-',\n",
       " '@bustedcoverage',\n",
       " '@bbcstrictly',\n",
       " '@jmroberts343jmr',\n",
       " 'itsabovemypaygradetocare',\n",
       " '@johnputch',\n",
       " '@sciencedaily',\n",
       " '@corporalfrisk',\n",
       " '@ramzinasir',\n",
       " 'sarcasmtweet',\n",
       " 'http://t.co/yeiqa3nfs1',\n",
       " 'http://t.co/ay20drme0l',\n",
       " '@jasminhadasah',\n",
       " 'phatsexyass',\n",
       " 'diktats',\n",
       " 'http://t.co/ct85mmtf5h',\n",
       " '@davidmwessel',\n",
       " 'howsam',\n",
       " '2b',\n",
       " '@denialfugly',\n",
       " '@mooglexox',\n",
       " 'http://t.co/jhuwhjleiu',\n",
       " 'bloodlinecontinues',\n",
       " '@docjp',\n",
       " '@davemyface',\n",
       " 'dyslipidemia',\n",
       " 'http://t.co/eciscydw4x',\n",
       " 'angryman',\n",
       " '@buzzfeeders',\n",
       " '@hudds1',\n",
       " 'legitreasontocomplain',\n",
       " 'on1meal',\n",
       " 'sprig\"—3',\n",
       " 'http://t.co/bzehrjzeks',\n",
       " 'smirking_face::cat_with_wry_smile',\n",
       " 'winterthought',\n",
       " '@theprojecttv',\n",
       " 'http://t.co/s7zhdhk5li',\n",
       " '@susancarlson111',\n",
       " 'omwtothemet',\n",
       " 'somuchbadnegativity',\n",
       " '-(|#shading',\n",
       " '@anamyid',\n",
       " 'serverlife',\n",
       " 'gimmesnow',\n",
       " '):',\n",
       " '@maraazzurra89',\n",
       " 'coldandflu',\n",
       " '@amazingphil',\n",
       " '@budweiser',\n",
       " '25',\n",
       " 'unamused_face:#basicbrianna',\n",
       " '@kirstiemallsopp',\n",
       " '@being_tiger',\n",
       " 'beaming_face_with_smiling_eyes',\n",
       " '@edballsmp',\n",
       " '@_jencita',\n",
       " '@oreospeedwagon',\n",
       " 'jeffroylat',\n",
       " 'http://t.co/ibxxkkrsto',\n",
       " 'http://t.co/peuz57xhmd',\n",
       " 'cpjzkegne7',\n",
       " 'http://t.co/iajmwd7yta',\n",
       " 'ptikeptpakfirst',\n",
       " '@singleology101',\n",
       " 'dashing_away::droplet::sweat_droplets',\n",
       " '12',\n",
       " 'http://t.co/ueun8ofick',\n",
       " '@di_mac1',\n",
       " 'anticonversionlaw',\n",
       " 'books::speak',\n",
       " 'http://t.co/3vwaobdxqm',\n",
       " 'advancedwarfare',\n",
       " '20',\n",
       " '@bencasselman',\n",
       " 'http://t.co/45xujgb7xf',\n",
       " 'not#had#reply',\n",
       " 'toeveryones',\n",
       " '9am',\n",
       " '@kojak_m',\n",
       " ':-p',\n",
       " 'http://t.co/4qjq9lm07u',\n",
       " '|#dislikes|msnbc',\n",
       " 'iaukea',\n",
       " 'http://t.co/9bjvi8v4hx',\n",
       " '@christiancaple',\n",
       " 'ithinkhehatesmesomedays',\n",
       " 'http://t.co/4l8inljg9n',\n",
       " '18',\n",
       " '#not#on#my#watch',\n",
       " 'morn!|#hah',\n",
       " 'http://t.co/szkw1assrt',\n",
       " 'sweater|#ebayipad|http://t.co',\n",
       " 'http://t.co/3nppprwgth',\n",
       " '@cryptodabbler',\n",
       " '3pt',\n",
       " '@cointelegraph',\n",
       " 'http://t.co/g2siem7zsv',\n",
       " 'democracymatters',\n",
       " '|snug',\n",
       " '+2.5',\n",
       " 'progoneate',\n",
       " '281',\n",
       " 'dcrising',\n",
       " 'orgabeh',\n",
       " 'http://t.co/spgfzziqlo',\n",
       " '8211',\n",
       " 'http://t.co/wih6mb1pfx',\n",
       " 'leavemealonemom',\n",
       " 'http://t.co/iclorq0p8d',\n",
       " '@jhill_official',\n",
       " '@crossfit',\n",
       " 'http://t.co/n5z2ixabq9',\n",
       " 'http://t.co/uisu74kglv',\n",
       " '10',\n",
       " '@holyroodmandy',\n",
       " '@classicdrwho247',\n",
       " 'asksuperwomanlive',\n",
       " '@susancalman',\n",
       " '@patrickrowe123',\n",
       " '@everydaysexism',\n",
       " '3rdeyebrand',\n",
       " '@sahelanth',\n",
       " 'myfairdaily',\n",
       " '1,176,120.90',\n",
       " '@allkpop',\n",
       " '@armshouseninja',\n",
       " 'http://t.co/snkptnhwj7',\n",
       " 'http://t.co/methuiaatr',\n",
       " '30',\n",
       " 'thanksfortheannoyingnote',\n",
       " 'waittng',\n",
       " '@pga_johndaly',\n",
       " 'http://t.co/neiozunbld',\n",
       " '@gemheartbeat',\n",
       " 'south32',\n",
       " '@juskoniall',\n",
       " '@them3blog',\n",
       " '@cnsnews',\n",
       " 'dogmatic~',\n",
       " 'http://t.co/z9ij5t2di9',\n",
       " '@eonline',\n",
       " '@stefjamesmusicã¢â\"¬â',\n",
       " 'bankopen',\n",
       " 'buffymusical',\n",
       " '@pier1984',\n",
       " '@gioneeindia',\n",
       " '500',\n",
       " '@charliejnwalker',\n",
       " 'http://t.co/sfr4lhy58',\n",
       " '@ok_magazine',\n",
       " 'unamused_face::oncoming_fist',\n",
       " '@evc_india',\n",
       " 'relationshipgoalsaccomplished',\n",
       " '@bigtant1986',\n",
       " '@billpowers9',\n",
       " 'seriously?to',\n",
       " 'http://t.co/pzxg42petx',\n",
       " '@fisolanydn',\n",
       " '@mediumvillain',\n",
       " '@blacktalkradio',\n",
       " '\"||north',\n",
       " \"you're\",\n",
       " '300',\n",
       " '@maddenthetwiggy',\n",
       " 'http://t.co/8qgrxywmne',\n",
       " 'http://t.co/p3ivori8k3',\n",
       " '04',\n",
       " 'conormcgregor',\n",
       " '@scottydons87',\n",
       " '@abelv03',\n",
       " '@simon_orourke',\n",
       " 'http://t.co/6rdixdnati',\n",
       " '8:30pm',\n",
       " 'beaming_face_with_smiling_eyes:|#me',\n",
       " 'http://t.co/fgocrs5kre',\n",
       " 'it!!!|http://t.co',\n",
       " 'http://t.co/omourvlfzt',\n",
       " 'notfortheintellectuals',\n",
       " '8a',\n",
       " '@wallace17_dakid',\n",
       " '@jeffdauler',\n",
       " '@playstation',\n",
       " 'http://t.co/x7rbgh13zk\":raising_hands::raising_hands:\"this',\n",
       " 'mt121',\n",
       " 'highpoints',\n",
       " 'itsmostlylove',\n",
       " '@sankrant',\n",
       " '@scottishfa',\n",
       " 'http://t.co/chteamozcc',\n",
       " 'beaming_face_with_smiling_eyes::beaming_face_with_smiling_eyes:#hawaii#trip#with#myfamiry#because#girlfriend',\n",
       " 'unamused_face::zzz',\n",
       " '@_kaypro',\n",
       " '@sunderlandafc',\n",
       " 'ullr',\n",
       " '@louisemensch',\n",
       " 'http://t.co/mvzxjl04pl||aw',\n",
       " '@steveh603',\n",
       " 'http://t.co/ak4baorhbc',\n",
       " '@coach_carts',\n",
       " '@foxdeportes',\n",
       " 'law&ordersvu',\n",
       " 'http://t.co/nxkfiwfs8i',\n",
       " '@floyding',\n",
       " 'store...http://t.co/axejcevq1e',\n",
       " '@knowonehome',\n",
       " '@maxbertellotti',\n",
       " 'http://t.co/vvawp30vua',\n",
       " '@dji',\n",
       " 'http://t.co/efbhmmlgew',\n",
       " '8',\n",
       " 'in2',\n",
       " '@topspinmonkeys',\n",
       " 'talkingabout',\n",
       " '@amaliearena',\n",
       " ':d',\n",
       " '@1071wnegradio',\n",
       " '@rebeldomm',\n",
       " 'neverarguewithfools',\n",
       " 'grinning_face_with_smiling_eyes::grinning_face_with_smiling_eyes',\n",
       " '-|gifted',\n",
       " 'smiling_face_with_sunglasses',\n",
       " 'sonot',\n",
       " '9gag',\n",
       " 'http://t.co/vzanl412f0',\n",
       " '@craigcalcaterra',\n",
       " '@randomnessisis',\n",
       " '@houselanguage',\n",
       " '@timowensby',\n",
       " '@jessicacameron',\n",
       " '⁰you',\n",
       " '......',\n",
       " 'http://t.co/aokwvdwkcc',\n",
       " 'twist|algorithms',\n",
       " 'lifesaved',\n",
       " '@kalobtaulien',\n",
       " 'thingsthatarewrong',\n",
       " '16',\n",
       " '@sqlsophist',\n",
       " '@lucasharriss',\n",
       " '1970',\n",
       " 'littletike',\n",
       " '4hoursleft',\n",
       " 'cliffhuxtable',\n",
       " '347',\n",
       " 'bringon2015',\n",
       " 'wine_glass::wine_glass',\n",
       " '@big6domino',\n",
       " 'denied.|#irony',\n",
       " '@carlscards',\n",
       " 'doin-',\n",
       " '<EOS>',\n",
       " '180',\n",
       " '@iphoneteam',\n",
       " '@kaylakrebs5',\n",
       " '@barryblackne',\n",
       " 'sevenfoot',\n",
       " 'http://t.co/jummwi0ayt',\n",
       " 'doumenturdeposit#not',\n",
       " 'enraged_face::enraged_face',\n",
       " 'coffee#1',\n",
       " '@bobclendenin',\n",
       " \"i'm\",\n",
       " 'http://t.co/8wdrilv4cp',\n",
       " '@tracieeeeee',\n",
       " 'avnish',\n",
       " '@lauratownx',\n",
       " 'hateschmoozing',\n",
       " '@dhawanvijoy',\n",
       " '@kiersttt22',\n",
       " '@jennybrew',\n",
       " 'ahsfreakshow',\n",
       " 'extenuating',\n",
       " \"'nuff\",\n",
       " '@chriscomben',\n",
       " 'rosebowl2015',\n",
       " 'communities-',\n",
       " '@marinaoloughlin',\n",
       " 'blasphemic',\n",
       " 'bornrisky',\n",
       " 'vice-#captain',\n",
       " '@peddraam',\n",
       " 'mehblog',\n",
       " '@jcolenc',\n",
       " '@daturbanconnect',\n",
       " 'http://t.co/pd4gucppkv',\n",
       " '12.4',\n",
       " 'smiling_face::smiling_face_with_smiling_eyes::flushed_face',\n",
       " 'https://t.co/w1p9bptdt3',\n",
       " '21',\n",
       " 'smiling_face_with_sunglasses::smiling_face_with_sunglasses::face_with_tears_of_joy::face_with_tears_of_joy',\n",
       " '@mrjoshhopkins',\n",
       " '@lewisdaay',\n",
       " 'http://t.co/h1cumqrffz',\n",
       " ':sparkling_heart',\n",
       " '@wykdwench',\n",
       " 'there#they',\n",
       " '@lr3031',\n",
       " '@thetweetofgod',\n",
       " 'http://t.co/serfvp5xzf',\n",
       " '@iamsteveharvey',\n",
       " '@hayekandhockey',\n",
       " 'http://t.co/s0msvqtpxu',\n",
       " '1/$12.5',\n",
       " '@justqueenvee',\n",
       " 'food|please',\n",
       " 'http://t.co/ir1k325loz',\n",
       " '@deepgreendesign',\n",
       " 'thumbs_down',\n",
       " '@tonyrohrs',\n",
       " 'http://t.co/zcptj6s5qn',\n",
       " 'http://t.co/yjqh8wj3ni',\n",
       " '1am',\n",
       " 'http://t.co/fzay2avx9',\n",
       " 'http://t.co/adkidknpid',\n",
       " 'notthepoint',\n",
       " '@justinbieber',\n",
       " '@gumlegs',\n",
       " '@sethedel',\n",
       " '@pembertonx',\n",
       " 'ccs_radio',\n",
       " '@selenagomez',\n",
       " '@jamieyuccas',\n",
       " 'laundrymatfun#lol#yesisaidnot#spincycle#youspinmeround#recordplayer#thatsmykid#80smusic#teachingth',\n",
       " '@wendywilliams',\n",
       " '@coutopanda',\n",
       " 'http://t.co/iogzrrsfgl',\n",
       " '@isajennie',\n",
       " 'ifussss',\n",
       " '@democratwire',\n",
       " 'http://t.co/liqfhfe0vl',\n",
       " '@braddrake23',\n",
       " '@obtoojiveforyou',\n",
       " 'liberal/#leftists',\n",
       " 'http://t.co/hhxqv23u65',\n",
       " 'friesenfamilychristmas',\n",
       " 'http://t.co/5kpdmwrwta',\n",
       " 'true!||call',\n",
       " 'http://t.co/2n3zjjzyqo',\n",
       " 'together!!:face_blowing_a_kiss::raising_hands::two_hearts',\n",
       " '@kafostersowell',\n",
       " '@senwarren',\n",
       " 'recycling_symbol::recycling_symbol',\n",
       " '@chixenman',\n",
       " 'http://t.co/usu8tlmo25',\n",
       " '@meyrickharris',\n",
       " 'harborne',\n",
       " '@mfouesneau',\n",
       " 'person_wearing_turban',\n",
       " 'enraged_face',\n",
       " 'http://t.co/tmr5zv5wu1',\n",
       " 'abbv',\n",
       " 'http://t.co/gq9zyivapn',\n",
       " '@wilw',\n",
       " '@imranj',\n",
       " '@star2000dancer',\n",
       " 'smiling_face_with_smiling_eyes',\n",
       " '@thestarphoenix',\n",
       " '@gilmanjames19',\n",
       " '@loic',\n",
       " 'gtaa',\n",
       " 'huweyyyyyy',\n",
       " 'https://t.co/koz2pho18y',\n",
       " '@sarapelissero',\n",
       " 'http://t.co/jmwkgojrnp',\n",
       " 'pro#idiot',\n",
       " 'mbloggers',\n",
       " '@airasia',\n",
       " 'http://t.co/s4pbuclkpm',\n",
       " 'expressionless_face::unamused_face',\n",
       " \"gamergate'rs\",\n",
       " 'http://t.co/mc9ebqjaqj',\n",
       " 'pearlburg',\n",
       " '@robertashton1',\n",
       " '@surianibteabu',\n",
       " 'clashupdate',\n",
       " '@garylineker',\n",
       " 'http://t.co/bigb5u44eh',\n",
       " 'sundayout',\n",
       " '@lexpersaud',\n",
       " 'welll',\n",
       " '50',\n",
       " '@cllr_alambritis',\n",
       " 'whatmorecouldaguyaskfor',\n",
       " 'ohgoditsfridayagain',\n",
       " '103',\n",
       " '5th',\n",
       " '2hrs',\n",
       " '@knoxnews',\n",
       " 'thumbs_up::thumbs_up',\n",
       " 'http://t.co/wmpp8iy3gw',\n",
       " '@scaretissuecom',\n",
       " 'http://t.co/d0taum1huh',\n",
       " 'http://t.co/mnim6qa5be',\n",
       " '@cupofjavi',\n",
       " 'cantopeneyes',\n",
       " '@being_akash',\n",
       " '@khushsundar',\n",
       " 'not|:church::bank::dollar_banknote',\n",
       " '@pgatour',\n",
       " '@mcquadejennica',\n",
       " 'http://t.co/xxpu1lb2ki',\n",
       " 'nursinglife',\n",
       " 'i8',\n",
       " '7|leave',\n",
       " '@realgilbert',\n",
       " '3d',\n",
       " 'http://t.co/l0ivdxhcmq',\n",
       " '@senrandpaul',\n",
       " 'ndtv||',\n",
       " 'http://t.co/ghwycsm5bw',\n",
       " 'sammycommentary',\n",
       " 'obsess.|just',\n",
       " '2:30',\n",
       " '@imalexbeamyrnot',\n",
       " 'http://t.co/6fg08vi4lp',\n",
       " '@bwinter',\n",
       " '@ramzaruglia',\n",
       " 'http://t.co/neazkoiv7',\n",
       " 'ionlygetbetter',\n",
       " '@rc1023fm',\n",
       " '@tombitt',\n",
       " '9',\n",
       " 'ilookreallygood',\n",
       " 'http://t.co/zwx1w60miu',\n",
       " 'decemberbessen',\n",
       " '@erickaajanae',\n",
       " 'wiseword',\n",
       " '@askplaystation',\n",
       " 'http://t.co/a4d9w4xpbs',\n",
       " '@petekopite',\n",
       " '@the_angry_ranga',\n",
       " '2a',\n",
       " '04:39pm',\n",
       " '36,000',\n",
       " 'crafts|please',\n",
       " 'thereasonfortheseason',\n",
       " 'http://t.co/oxybfwwwb6',\n",
       " 'sticktoparenting',\n",
       " '@askegg',\n",
       " '@espngolic',\n",
       " '@littlelizardg',\n",
       " 'http://t.co/wzlnwsrjel',\n",
       " '@railwaysloth',\n",
       " 'http://t.co/ozt0e541p9',\n",
       " 'http://t.co/qx4ew1q2co',\n",
       " '@mohammadfarooq',\n",
       " '@cressy_36',\n",
       " 'thebullscanhavehim',\n",
       " ...}"
      ]
     },
     "execution_count": 790,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "not_training_data = dev_sentences + test_sentences\n",
    "my_tokenizer = Tokenizer()\n",
    "\n",
    "tokenized_data = my_tokenizer.tokenize(not_training_data)\n",
    "not_training_vocab = set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws])\n",
    "\n",
    "not_training_oovs = set(oovs).intersection(not_training_vocab)\n",
    "not_training_oovs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 792,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6784, 2120)"
      ]
     },
     "execution_count": 792,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(not_training_vocab), len(not_training_oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15302, 8518)"
      ]
     },
     "execution_count": 794,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), len(set(vocab) - not_training_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 698,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 698,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05115519,  2.79060194,  1.74904423, ...,  0.90218995,\n",
       "        -0.07823642,  0.46452929],\n",
       "       [ 0.03957311, -0.08359327,  0.07810808, ..., -1.1645801 ,\n",
       "        -1.47297465,  1.90070732],\n",
       "       [ 0.62807933,  0.95442775, -0.89309501, ..., -1.15517324,\n",
       "        -0.74455474, -0.79810859],\n",
       "       ...,\n",
       "       [ 1.49882801,  0.70325473, -1.13812968, ...,  0.54420823,\n",
       "        -0.29446765, -0.33132347],\n",
       "       [-1.28244904, -0.60616415,  0.90391816, ..., -0.01481464,\n",
       "        -0.34305424, -2.27542031],\n",
       "       [-0.27966446, -0.14053061, -0.39564932, ...,  1.14049864,\n",
       "        -0.76508639,  1.06653365]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05115519,  2.79060194,  1.74904423, ...,  0.90218995,\n",
       "        -0.07823642,  0.46452929],\n",
       "       [ 0.03957311, -0.08359327,  0.07810808, ..., -1.1645801 ,\n",
       "        -1.47297465,  1.90070732],\n",
       "       [ 0.62807933,  0.95442775, -0.89309501, ..., -1.15517324,\n",
       "        -0.74455474, -0.79810859],\n",
       "       ...,\n",
       "       [ 1.49882801,  0.70325473, -1.13812968, ...,  0.54420823,\n",
       "        -0.29446765, -0.33132347],\n",
       "       [-1.28244904, -0.60616415,  0.90391816, ..., -0.01481464,\n",
       "        -0.34305424, -2.27542031],\n",
       "       [-0.27966446, -0.14053061, -0.39564932, ...,  1.14049864,\n",
       "        -0.76508639,  1.06653365]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.05115519,  2.79060194,  1.74904423, ...,  0.90218995,\n",
       "        -0.07823642,  0.46452929],\n",
       "       [ 0.03957311, -0.08359327,  0.07810808, ..., -1.1645801 ,\n",
       "        -1.47297465,  1.90070732],\n",
       "       [ 0.62807933,  0.95442775, -0.89309501, ..., -1.15517324,\n",
       "        -0.74455474, -0.79810859],\n",
       "       ...,\n",
       "       [ 0.72198955, -2.17146252,  0.0466316 , ...,  0.17704579,\n",
       "        -1.78284933, -0.21232404],\n",
       "       [-2.08491991, -1.31290281,  1.06203141, ..., -0.36402803,\n",
       "        -0.35470426, -0.40404115],\n",
       "       [-0.39347399,  0.65912157,  0.28453421, ...,  0.17566116,\n",
       "        -0.99604728, -0.5599636 ]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.Wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.614 ,  1.0055, -0.1676, ..., -1.4183,  0.728 , -2.0562],\n",
       "       [-0.48  , -0.7523, -0.9078, ..., -0.0931,  0.1954, -0.9103],\n",
       "       [-0.893 ,  0.1351, -1.06  , ..., -1.2487,  1.2845, -1.5094],\n",
       "       ...,\n",
       "       [-0.4075,  0.6146,  0.6026, ..., -0.1131, -0.2829, -1.1643],\n",
       "       [-0.7772, -0.1522,  2.1256, ...,  0.3602, -0.4155, -0.0238],\n",
       "       [-1.212 , -1.4438,  0.4278, ..., -0.9146,  0.6358, -0.5758]])"
      ]
     },
     "execution_count": 859,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.Wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.0009,  0.272 ,  0.4744,  0.075 ,  0.0352, -0.107 , -0.552 ,\n",
       "         0.1838, -0.119 ,  0.0983,  0.2458,  0.2809,  0.2147, -0.0157,\n",
       "         0.3402, -0.1941]])"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.forward(embedding_layer.forward(batch_sentences_train[0][:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24, 19,  7, 13, 31,  8, 33, 12, 21, 26, 11, 11, 22, 16, 20, 13, 20,\n",
       "       38, 31, 18, 10, 35, 17, 31, 12, 12, 17, 17, 23, 12, 14, 19, 10, 13,\n",
       "       21, 30, 18, 16, 24, 21, 18, 37, 17, 25, 22, 10, 24, 29, 17, 21, 19,\n",
       "       24, 12, 14, 31, 26, 14, 27, 34, 12, 25, 11, 25, 30, 12, 30, 10, 17,\n",
       "       34, 24, 26, 14, 25, 29, 23, 19, 22, 22, 24, 22, 15, 31, 33, 13, 24,\n",
       "       24, 26, 22, 13, 23, 17, 23, 18, 28, 29, 26, 23, 17, 16, 28, 28, 17,\n",
       "       13, 30, 20, 30, 22, 32, 18, 30, 14, 22, 24, 12, 25, 25, 19, 13,  9,\n",
       "       22, 19,  9, 16, 28, 40, 13, 25, 18, 31, 12, 31, 33, 11, 25, 15, 32,\n",
       "       21,  8, 30, 25, 17, 16, 23, 29, 17, 28, 32, 30, 12, 24, 17, 22, 38,\n",
       "       13, 13, 25, 20, 19, 21, 17, 32, 17, 27, 11, 16, 22, 24, 29, 22, 29,\n",
       "       15, 10, 22, 20, 13, 12, 13, 14,  9, 23, 24, 29, 35, 21, 18,  8, 11,\n",
       "       15, 27, 26, 19, 14, 28, 21, 11, 10, 28, 17, 35, 20, 30, 21, 41, 16,\n",
       "       19, 21, 23, 18, 32, 15,  9, 10, 25, 23, 16, 19, 20, 31, 24, 20,  9,\n",
       "       22, 26,  8, 36, 30, 16, 18, 24, 31, 26, 31, 17, 12, 13, 26, 30, 22,\n",
       "       18, 15, 24, 23, 33, 26, 26, 26, 22, 21, 24, 18, 17, 15, 19, 12, 15,\n",
       "       22, 12, 10, 31, 30, 31, 22, 17, 23, 27, 27, 19, 26, 30, 14, 22, 19,\n",
       "       16, 16, 35, 24,  9, 31, 16, 28, 14, 35, 17, 18, 20, 14, 26, 21, 28,\n",
       "       26, 10, 33, 29, 17, 11, 23, 19, 21, 29, 22, 22, 13, 22,  6, 23, 15,\n",
       "       15, 16, 16, 20, 28, 23, 29, 24, 30, 25, 27, 13, 19, 26, 24, 26, 25,\n",
       "       14, 30, 13, 33, 30, 36, 30, 24, 21, 11, 25, 22, 14, 20, 25, 23, 27,\n",
       "       12, 26, 15, 16, 28, 28, 13, 22, 27, 25, 22, 17, 12, 10, 15, 16, 27,\n",
       "       30, 26, 23, 18, 32, 33, 12, 35, 14, 12, 19, 21, 10, 23, 15, 15, 22,\n",
       "       36,  9,  7, 39, 12, 17, 17, 11, 26, 28, 19,  8, 13, 33, 16, 31, 23,\n",
       "       21, 19, 27, 12, 35, 20, 26, 28, 16, 19, 13, 19, 34, 27, 20, 13, 10,\n",
       "       19, 17, 17, 24, 16, 26, 25, 20, 18, 30, 27, 16, 23, 10, 26, 14, 27,\n",
       "       23, 12, 11, 22, 18, 30, 26, 18, 24, 11, 21, 19, 17, 30, 11, 13, 19,\n",
       "       17, 17, 26, 25, 29, 31, 13, 11, 19,  7, 29, 34, 22, 17, 22, 13, 22,\n",
       "       14, 15, 11, 17, 18, 25, 14, 13,  9, 20, 16, 33, 10, 23, 22, 13, 29,\n",
       "       16, 24, 22, 31, 11, 20, 18, 28,  8, 31, 29, 18, 10, 29, 16, 14, 27,\n",
       "       22, 28, 35, 34, 19, 20, 18, 16, 22, 27, 10, 32, 20, 15, 11, 17, 21,\n",
       "       35, 16, 13, 19, 17, 13, 20, 20, 17, 22, 24, 20, 21, 12, 28, 27, 20,\n",
       "       16, 23, 10, 22, 22, 30, 17, 30, 11, 15, 12, 40, 23, 18, 24, 11, 13,\n",
       "       19, 19, 10, 12, 17, 12, 34, 17, 24, 36, 24, 30, 26, 25, 25, 24, 11,\n",
       "       11, 20, 26, 17, 22, 27, 12, 29, 15, 30, 15, 31, 23, 29, 20,  9, 30,\n",
       "       25, 17, 14, 28, 16, 26, 23, 33, 28, 23, 28, 10, 31, 28, 32, 16, 30,\n",
       "       17, 15, 18, 17, 24, 24, 19, 27, 29, 21, 14, 21, 26, 26, 14, 14, 13,\n",
       "       15, 25, 21, 30, 32, 15, 19, 18, 19, 21, 29, 12, 17, 34, 21, 18, 15,\n",
       "       22, 19, 14, 24, 18, 27, 26, 28, 29, 10, 34, 20, 25, 15, 11, 25, 10,\n",
       "       25, 22, 16, 27, 27, 23, 31, 15, 29, 16, 11, 24, 15, 16, 14, 14, 16,\n",
       "       28, 10, 25, 43, 25, 29, 37, 22, 32, 17, 30, 17, 17, 12, 17, 32, 11,\n",
       "       24,  8, 17, 30, 21, 33, 21,  8, 26, 33, 17, 23, 27, 12, 32, 24, 19,\n",
       "       39, 25, 16, 11, 17, 16, 10, 18, 16, 34, 11, 19, 29, 28,  9, 23, 24,\n",
       "       13, 17, 27, 27, 13, 23, 15, 12, 19, 12, 16, 27,  5, 25,  5, 15, 22,\n",
       "       23, 27, 16, 12,  8, 17, 16,  9, 29, 13, 16, 15, 24, 12, 21, 10, 51,\n",
       "       24, 27, 29, 28, 32, 22, 20, 13, 31, 27, 11, 16, 12, 31, 11, 22, 26,\n",
       "       12, 24, 19, 20, 34, 23, 23, 29, 20, 18, 16, 20, 23, 17, 29, 36, 20,\n",
       "       20, 30])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.7545,  0.0282, -0.1592, ..., -0.4149, -0.0049, -0.1407],\n",
       "        ...,\n",
       "        [-0.0466,  0.3701, -0.4454, ...,  0.0014, -0.0075, -0.0044],\n",
       "        [ 0.5875,  0.1914, -0.0786, ...,  0.0058, -0.081 , -0.1206],\n",
       "        [ 0.6102,  0.3939, -0.0117, ...,  0.0298, -0.1872, -0.0031]],\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.4924, -0.2652, -0.3919, ..., -0.338 , -0.1627,  0.2448],\n",
       "        ...,\n",
       "        [ 0.2651,  0.1682, -0.4598, ..., -0.0756, -0.8167, -0.0436],\n",
       "        [ 0.0166, -0.6327, -0.148 , ..., -0.0053, -0.7913, -0.3015],\n",
       "        [ 0.0409, -0.648 , -0.0991, ..., -0.024 , -0.016 ,  0.3617]],\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.1133, -0.1218, -0.0771, ..., -0.3863, -0.074 ,  0.3573],\n",
       "        ...,\n",
       "        [ 0.3176, -0.0534,  0.5289, ..., -0.083 , -0.208 ,  0.6293],\n",
       "        [ 0.2903, -0.6398,  0.9245, ..., -0.0865, -0.0485,  0.1043],\n",
       "        [-0.0357, -0.9193,  0.8956, ..., -0.1287, -0.0447,  0.8277]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.167 , -0.0146, -0.2764, ..., -0.776 , -0.0324,  0.3972],\n",
       "        ...,\n",
       "        [ 0.7607, -0.1174, -0.2926, ...,  0.328 , -0.1697, -0.1485],\n",
       "        [ 0.6052,  0.5122, -0.1473, ...,  0.2282, -0.1209,  0.0433],\n",
       "        [ 0.6595, -0.2331, -0.1046, ...,  0.2728, -0.0481,  0.01  ]],\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.8097,  0.0135, -0.1257, ..., -0.2408, -0.0338,  0.0874],\n",
       "        ...,\n",
       "        [-0.0133, -0.0051, -0.1693, ..., -0.0366, -0.0049,  0.148 ],\n",
       "        [-0.0348, -0.0008, -0.0787, ..., -0.006 , -0.0433,  0.2753],\n",
       "        [ 0.0106, -0.0012, -0.038 , ..., -0.0968, -0.001 ,  0.1003]],\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    , ...,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.5149, -0.0346, -0.3286, ..., -0.1882, -0.0276,  0.1175],\n",
       "        [ 0.7545,  0.0282, -0.1592, ..., -0.4149, -0.0049, -0.1407],\n",
       "        ...,\n",
       "        [ 0.0167,  0.0078, -0.7625, ...,  0.2705, -0.0036,  0.96  ],\n",
       "        [ 0.2067,  0.002 , -0.8127, ...,  0.5223, -0.1803,  0.8994],\n",
       "        [ 0.0854, -0.0004, -0.6709, ...,  0.1068, -0.0021,  0.9789]]])"
      ]
     },
     "execution_count": 883,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embeddings embedding_dim: 10 num_embeddings: 5"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer = Embeddings(5, 10, 0)\n",
    "embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [1, 4, 1, 0]])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,3,0,0], [1,4,1,0]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[[ 0.11844567,  0.49995652, -0.65802771, -1.20548665,\n",
       "           0.21329025,  0.58381855, -0.16526934, -0.25223838,\n",
       "           0.76283889,  0.94854758],\n",
       "         [-1.51672031,  0.03719266, -0.44633685,  2.44429478,\n",
       "           1.71287478, -0.76141086,  1.07287826, -0.21405207,\n",
       "          -1.29394351,  0.29765764],\n",
       "         [ 0.61638133, -0.96546929, -0.10739356,  0.74669643,\n",
       "           0.27681166,  1.67266701,  0.72222543,  0.93011466,\n",
       "           1.28923875, -2.84106437],\n",
       "         [ 0.61638133, -0.96546929, -0.10739356,  0.74669643,\n",
       "           0.27681166,  1.67266701,  0.72222543,  0.93011466,\n",
       "           1.28923875, -2.84106437]],\n",
       " \n",
       "        [[-1.52225956, -0.99993144,  0.31207765, -1.16411845,\n",
       "           0.95962902, -1.11016667,  0.27763399,  0.25024548,\n",
       "           0.51261377, -1.12282121],\n",
       "         [-1.51425629, -0.2689891 ,  0.7137898 ,  0.89214974,\n",
       "          -0.48302   ,  0.94458768, -0.37389572, -0.14598221,\n",
       "          -0.59578975, -0.73285059],\n",
       "         [-1.52225956, -0.99993144,  0.31207765, -1.16411845,\n",
       "           0.95962902, -1.11016667,  0.27763399,  0.25024548,\n",
       "           0.51261377, -1.12282121],\n",
       "         [ 0.61638133, -0.96546929, -0.10739356,  0.74669643,\n",
       "           0.27681166,  1.67266701,  0.72222543,  0.93011466,\n",
       "           1.28923875, -2.84106437]]]),\n",
       " 'seq_lens': array([2, 3])}"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out1 = embedding_layer.forward(X)\n",
    "out1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = LSTM(4, return_seq=True, return_mask=True)(embedding_layer)\n",
    "lstm = LSTM(4, return_seq=False)(lstm1)\n",
    "dense = Dense(2)(lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.inp_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 944,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = np.ones((5, 4, 3))\n",
    "# X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.    , 0.    , 0.    , 0.    ],\n",
       "        [0.6511, 0.4577, 0.9708, 0.2667],\n",
       "        [0.0445, 0.133 , 0.5706, 0.6013],\n",
       "        [0.018 , 0.9978, 0.1421, 0.0926],\n",
       "        [0.0074, 0.9998, 0.4178, 0.1584]],\n",
       "\n",
       "       [[0.    , 0.    , 0.    , 0.    ],\n",
       "        [0.0766, 0.0793, 0.9569, 0.7063],\n",
       "        [0.0442, 0.9916, 0.0006, 0.0399],\n",
       "        [0.0294, 0.403 , 0.9871, 0.7899],\n",
       "        [0.0937, 0.9995, 0.2483, 0.0019]]])"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.fg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[[ 3.74326341e-01,  1.09327787e-01, -2.01756594e-02,\n",
       "           7.11266094e-02],\n",
       "         [ 2.76879383e-02, -1.26430568e-01,  1.39025959e-02,\n",
       "          -6.55696046e-02],\n",
       "         [-6.52705451e-01, -8.33636621e-01, -4.68831358e-01,\n",
       "           6.32572510e-03],\n",
       "         [-9.03486253e-01, -9.51213402e-01, -8.68561298e-01,\n",
       "           8.78239453e-03]],\n",
       " \n",
       "        [[ 3.92885282e-02, -6.53056042e-02, -5.39128568e-01,\n",
       "           1.42195093e-04],\n",
       "         [-5.36765554e-01, -7.64438079e-01, -1.20788970e-01,\n",
       "           2.18462473e-02],\n",
       "         [ 3.88728567e-02, -1.23081991e-01, -8.31529499e-01,\n",
       "           2.04708422e-02],\n",
       "         [-7.39809751e-01, -8.95744397e-01, -9.66949000e-01,\n",
       "           1.07063025e-02]]]),\n",
       " 'seq_lens': array([2, 3])}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = lstm1.forward(out1)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 3.74326341e-01,  1.09327787e-01, -2.01756594e-02,\n",
       "          7.11266094e-02],\n",
       "        [ 2.76879383e-02, -1.26430568e-01,  1.39025959e-02,\n",
       "         -6.55696046e-02],\n",
       "        [-6.52705451e-01, -8.33636621e-01, -4.68831358e-01,\n",
       "          6.32572510e-03],\n",
       "        [-9.03486253e-01, -9.51213402e-01, -8.68561298e-01,\n",
       "          8.78239453e-03]],\n",
       "\n",
       "       [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "          0.00000000e+00],\n",
       "        [ 3.92885282e-02, -6.53056042e-02, -5.39128568e-01,\n",
       "          1.42195093e-04],\n",
       "        [-5.36765554e-01, -7.64438079e-01, -1.20788970e-01,\n",
       "          2.18462473e-02],\n",
       "        [ 3.88728567e-02, -1.23081991e-01, -8.31529499e-01,\n",
       "          2.04708422e-02],\n",
       "        [-7.39809751e-01, -8.95744397e-01, -9.66949000e-01,\n",
       "          1.07063025e-02]]])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm1.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [1, 4, 1, 0]])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.09760457, -0.13338779,  0.02814205, -0.14806707],\n",
       "       [ 0.34470959,  0.23761088, -0.17409077,  0.23676048]])"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out3 = lstm.forward(out)\n",
    "out3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.07855371, -0.14074963,  0.16732286, -0.13013733],\n",
       "        [-0.09760457, -0.13338779,  0.02814205, -0.14806707],\n",
       "        [ 0.03975932,  0.1266031 , -0.13598183,  0.20835219],\n",
       "        [ 0.0561025 ,  0.10093059, -0.1341875 ,  0.27669279]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.13433303,  0.2249573 , -0.09394001, -0.01706918],\n",
       "        [ 0.13240294,  0.11801888, -0.1209671 ,  0.23776542],\n",
       "        [ 0.34470959,  0.23761088, -0.17409077,  0.23676048],\n",
       "        [ 0.12173959,  0.10047364, -0.10940592,  0.23454546]]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.11223215, -0.03931857],\n",
       "       [-0.16044303,  0.77155521]])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = dense.forward(out3)\n",
    "out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX\n",
      "[[[ 0.          0.          0.          0.        ]\n",
      "  [ 0.05332589  0.00516852  0.06965946 -0.01055025]\n",
      "  [ 0.03592446 -0.0117603   0.03784639 -0.01498404]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.        ]\n",
      "  [-0.01275348 -0.05692982  0.03757692  0.01166092]\n",
      "  [ 0.17349736 -0.42322456 -0.32163424  0.0360653 ]\n",
      "  [-0.9429311  -0.30329168  0.04347839  0.44282734]\n",
      "  [ 0.          0.          0.          0.        ]]]\n",
      "dstate\n",
      "[[[-0.00261688 -0.018689   -0.01478616 -0.01906517]\n",
      "  [-0.00419815 -0.06514418 -0.01982833 -0.02856457]\n",
      "  [ 0.04522555 -0.08438495 -0.03783448 -0.02488617]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.04378892 -0.01372805  0.01033982  0.01623359]\n",
      "  [-0.09185422 -0.07548797  0.01281539  0.03317353]\n",
      "  [-0.27016119 -0.02206315 -0.59054194  0.58491162]\n",
      "  [-0.3024206  -0.53490295 -0.7187889   0.69851015]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "dX, dW, db = lstm.backward(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dX\n",
      "[[[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.00218676  0.00632558 -0.0044597   0.00114102 -0.00377907\n",
      "    0.02417695  0.00120844 -0.0032456   0.00779047 -0.01200078]\n",
      "  [ 0.00457815 -0.00165317  0.00565791  0.00260445  0.00158525\n",
      "    0.00437805 -0.00157866  0.00077636  0.00073112 -0.00178632]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]]\n",
      "\n",
      " [[ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]\n",
      "  [ 0.00177688  0.02925502  0.01909082  0.00404067  0.01567801\n",
      "    0.05020792 -0.0291943   0.01623197 -0.01736629 -0.04664208]\n",
      "  [ 0.07661261 -0.04912754  0.02399619  0.09333513 -0.05932359\n",
      "   -0.0087092   0.05002928 -0.03507111  0.08198772 -0.00519837]\n",
      "  [-0.06099801 -0.00779837  0.02564258  0.04572204 -0.06025049\n",
      "    0.0363533   0.0241812   0.07375339  0.02334509 -0.02965456]\n",
      "  [ 0.          0.          0.          0.          0.\n",
      "    0.          0.          0.          0.          0.        ]]]\n",
      "dstate\n",
      "[[[ 0.00653201  0.00135336  0.0019829  -0.00674125]\n",
      "  [ 0.03020532  0.00221228  0.01843004 -0.01927248]\n",
      "  [ 0.00394092 -0.00416737  0.00425811 -0.00957834]\n",
      "  [ 0.          0.          0.          0.        ]\n",
      "  [ 0.          0.          0.          0.        ]]\n",
      "\n",
      " [[-0.00558645 -0.02165051 -0.01228683  0.00820025]\n",
      "  [-0.05022164 -0.10004271 -0.01338922  0.01529037]\n",
      "  [ 0.0476778  -0.14500395 -0.00930259  0.0254769 ]\n",
      "  [-0.81080316 -0.05587198  0.00238771  0.04783393]\n",
      "  [ 0.          0.          0.          0.        ]]]\n"
     ]
    }
   ],
   "source": [
    "dX, dW, db = lstm1.backward(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.13776511,  0.02097367,  0.40883244, -0.26052464],\n",
       "        [-0.28063548,  0.00266169,  0.86347146, -0.29904221],\n",
       "        [-0.69821838, -0.50917675,  0.84450935, -0.37113322],\n",
       "        [-0.78662037, -0.35453346,  0.89423365, -0.28881855]],\n",
       "\n",
       "       [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [-0.69887257, -0.27756377,  0.06375953,  0.18895972],\n",
       "        [-0.85692807, -0.03247021,  0.70473237, -0.56481148],\n",
       "        [-0.94235021, -0.34661484,  0.64816491, -0.47360723],\n",
       "        [-0.82331877, -0.76438959,  0.92088445, -0.31302486]]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [-0.0036, -0.582 , -0.8003,  0.1991],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ]],\n",
       "\n",
       "       [[ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ],\n",
       "        [-0.0128, -0.0181,  0.1125, -0.7442],\n",
       "        [ 0.    ,  0.    ,  0.    ,  0.    ]]])"
      ]
     },
     "execution_count": 612,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO_ = np.zeros_like(lstm.out)\n",
    "dO_[np.arange(out1['input_ids'].shape[0]), lstm.mask, :] = out\n",
    "dO_\n",
    "# dO = dO_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([[ 0.        , -0.05922114,  0.00218676,  0.00457815,  0.07661261],\n",
       "        [ 0.        ,  0.02145665,  0.00632558, -0.00165317, -0.04912754],\n",
       "        [ 0.        ,  0.0447334 , -0.0044597 ,  0.00565791,  0.02399619],\n",
       "        [ 0.        ,  0.04976271,  0.00114102,  0.00260445,  0.09333513],\n",
       "        [ 0.        , -0.04457249, -0.00377907,  0.00158525, -0.05932359],\n",
       "        [ 0.        ,  0.08656122,  0.02417695,  0.00437805, -0.0087092 ],\n",
       "        [ 0.        , -0.0050131 ,  0.00120844, -0.00157866,  0.05002928],\n",
       "        [ 0.        ,  0.08998536, -0.0032456 ,  0.00077636, -0.03507111],\n",
       "        [ 0.        ,  0.00597879,  0.00779047,  0.00073112,  0.08198772],\n",
       "        [ 0.        , -0.07629663, -0.01200078, -0.00178632, -0.00519837]]),\n",
       " None)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.backward(dX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [1, 4, 1, 0]])"
      ]
     },
     "execution_count": 641,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.00218676,  0.00632558, -0.0044597 ,  0.00114102,\n",
       "         -0.00377907,  0.02417695,  0.00120844, -0.0032456 ,\n",
       "          0.00779047, -0.01200078],\n",
       "        [ 0.00457815, -0.00165317,  0.00565791,  0.00260445,\n",
       "          0.00158525,  0.00437805, -0.00157866,  0.00077636,\n",
       "          0.00073112, -0.00178632],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ]],\n",
       "\n",
       "       [[ 0.00177688,  0.02925502,  0.01909082,  0.00404067,\n",
       "          0.01567801,  0.05020792, -0.0291943 ,  0.01623197,\n",
       "         -0.01736629, -0.04664208],\n",
       "        [ 0.07661261, -0.04912754,  0.02399619,  0.09333513,\n",
       "         -0.05932359, -0.0087092 ,  0.05002928, -0.03507111,\n",
       "          0.08198772, -0.00519837],\n",
       "        [-0.06099801, -0.00779837,  0.02564258,  0.04572204,\n",
       "         -0.06025049,  0.0363533 ,  0.0241812 ,  0.07375339,\n",
       "          0.02334509, -0.02965456],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ]]])"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.34236954e-05,  3.48001222e-03, -3.01183044e-04,\n",
       "        -1.39723269e-04, -3.23882547e-03,  6.49948497e-04,\n",
       "         6.11086862e-03, -5.69352317e-03],\n",
       "       [ 6.82570249e-03,  1.81512230e-02,  5.16372383e-04,\n",
       "         1.47822349e-02, -3.37038306e-02, -9.55732222e-03,\n",
       "         4.37065888e-02, -2.04024695e-02],\n",
       "       [ 6.80646142e-03,  3.39200561e-02,  2.71320253e-02,\n",
       "         3.31304287e-02, -1.53301216e-01, -5.22977361e-02,\n",
       "         7.82465163e-02, -4.94589470e-02],\n",
       "       [-5.97894992e-03, -1.61054161e-02, -7.73983436e-03,\n",
       "        -1.42489793e-02,  4.97698560e-02,  1.17907714e-02,\n",
       "        -4.00771246e-02,  2.71915634e-02],\n",
       "       [ 1.17544666e-03,  6.84595295e-04, -4.62125066e-04,\n",
       "         8.17768813e-04, -5.57717133e-03,  1.90028548e-04,\n",
       "         7.13945130e-03, -6.02578380e-03],\n",
       "       [ 7.30720765e-03,  1.56494740e-02,  2.49934283e-03,\n",
       "         1.17733442e-02, -3.30604011e-02, -1.22425306e-02,\n",
       "         4.10656120e-02, -2.10979068e-02],\n",
       "       [ 1.90958911e-03,  2.03087906e-02,  2.14714604e-02,\n",
       "         1.89371996e-02, -1.09960518e-01, -3.72398611e-02,\n",
       "         4.65828822e-02, -2.63604029e-02],\n",
       "       [-5.58890477e-03, -1.69246734e-02, -6.01590931e-03,\n",
       "        -1.34258239e-02,  4.66733738e-02,  1.25002487e-02,\n",
       "        -3.89018983e-02,  2.53425933e-02],\n",
       "       [ 3.27918404e-03,  6.73154710e-03,  3.09103375e-03,\n",
       "         8.62480096e-03, -2.84828472e-02, -1.13510705e-03,\n",
       "         2.62366661e-02, -1.98946958e-02],\n",
       "       [ 1.73451445e-02,  6.11585708e-02,  1.41572988e-02,\n",
       "         4.87293886e-02, -1.57266303e-01, -6.08550470e-02,\n",
       "         1.36768488e-01, -7.95316429e-02],\n",
       "       [ 2.07045226e-03,  1.18188872e-02,  1.29181665e-02,\n",
       "         1.39435939e-02, -5.44362952e-02, -1.89315249e-02,\n",
       "         2.48013759e-02, -3.00038764e-02],\n",
       "       [-4.04536651e-03, -1.58657299e-02, -7.26919316e-03,\n",
       "        -1.13126249e-02,  4.23846820e-02,  1.58256208e-02,\n",
       "        -2.93260218e-02,  1.91352268e-02],\n",
       "       [ 4.60617198e-03,  9.24963777e-03,  1.43515869e-03,\n",
       "         9.63466303e-03, -3.74906911e-02, -4.69109959e-03,\n",
       "         3.89077914e-02, -2.47124491e-02],\n",
       "       [ 2.30624584e-02,  5.48856922e-02, -1.17386142e-02,\n",
       "         3.53730366e-02, -6.14601230e-02, -3.59975603e-02,\n",
       "         1.26381092e-01, -6.46677528e-02],\n",
       "       [ 3.53967713e-02,  8.98815056e-02,  3.60838886e-02,\n",
       "         8.68183347e-02, -2.96355017e-01, -4.81862776e-02,\n",
       "         2.64034064e-01, -1.70437220e-01],\n",
       "       [-1.10129252e-02, -1.90768484e-02,  8.54801378e-04,\n",
       "        -1.17984494e-02,  5.70493964e-02,  1.33530062e-02,\n",
       "        -6.55855221e-02,  1.77453192e-02]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01220342,  0.0572976 ,  0.1649863 , -0.06155899],\n",
       "       [ 0.01121735,  0.05274276,  0.12200713, -0.05893465],\n",
       "       [ 0.04408651,  0.19130427,  0.04313542, -0.04690746],\n",
       "       [ 0.08806456,  0.12116166,  0.39583877, -0.12531922]])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.0001, 10.0024,  9.9972, 10.    , 10.0043,  9.999 , 10.0012,\n",
       "         9.9973,  9.9998,  9.9999, 10.0034, 10.0084,  9.9988,  9.9993],\n",
       "       [ 9.9997,  9.9998, 10.0011,  9.9998, 10.0001,  9.9994,  9.9997,\n",
       "         9.9991,  9.9992, 10.0031,  9.9988,  9.9995,  9.9995, 10.0017],\n",
       "       [10.0035, 10.0035, 10.0214, 10.0288, 10.0168, 10.0257,  9.9654,\n",
       "        10.0119,  9.8888, 10.0588, 10.0023,  9.9728,  9.9593,  9.9891],\n",
       "       [ 9.9995,  9.9996,  9.9998, 10.001 , 10.0009, 10.0004,  9.9992,\n",
       "         9.9994, 10.    ,  9.9999,  9.9999,  9.9989, 10.0006,  9.999 ]])"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wf = dW[:4, :]\n",
    "Wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True, precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 841,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wf += 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 843,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = dW + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 844,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[110.0001, 110.0024, 109.9972, 110.    , 110.0043, 109.999 ,\n",
       "        110.0012, 109.9973, 109.9998, 109.9999, 110.0034, 110.0084,\n",
       "        109.9988, 109.9993],\n",
       "       [109.9997, 109.9998, 110.0011, 109.9998, 110.0001, 109.9994,\n",
       "        109.9997, 109.9991, 109.9992, 110.0031, 109.9988, 109.9995,\n",
       "        109.9995, 110.0017],\n",
       "       [110.0035, 110.0035, 110.0214, 110.0288, 110.0168, 110.0257,\n",
       "        109.9654, 110.0119, 109.8888, 110.0588, 110.0023, 109.9728,\n",
       "        109.9593, 109.9891],\n",
       "       [109.9995, 109.9996, 109.9998, 110.001 , 110.0009, 110.0004,\n",
       "        109.9992, 109.9994, 110.    , 109.9999, 109.9999, 109.9989,\n",
       "        110.0006, 109.999 ],\n",
       "       [100.0001,  99.9998,  99.9997,  99.9995,  99.998 , 100.0013,\n",
       "         99.9999, 100.0027,  99.9964, 100.0023,  99.9978,  99.9966,\n",
       "         99.9999,  99.9987],\n",
       "       [100.0002, 100.0041, 100.0113,  99.9986,  99.9839, 100.0199,\n",
       "         99.991 , 100.0184, 100.0339,  99.9832,  99.9831,  99.9619,\n",
       "         99.9924,  99.9719],\n",
       "       [100.0017, 100.0035, 100.0184, 100.0182, 100.0024, 100.019 ,\n",
       "         99.9784, 100.0161,  99.9329, 100.0381,  99.992 ,  99.9611,\n",
       "         99.9749,  99.9905],\n",
       "       [100.0007,  99.9995,  99.9998,  99.9995,  99.9976, 100.0002,\n",
       "         99.9991, 100.0004,  99.9987,  99.9986, 100.0003,  99.9984,\n",
       "         99.9994, 100.0006],\n",
       "       [ 99.994 , 100.0012, 100.0219, 100.0053,  99.9723, 100.0122,\n",
       "         99.9912, 100.0294,  99.9912, 100.0073,  99.971 ,  99.9289,\n",
       "        100.0024,  99.9883],\n",
       "       [ 99.9987, 100.0222, 100.0004, 100.0008, 100.0292, 100.079 ,\n",
       "         99.9416, 100.0025, 100.0998,  99.9415, 100.022 , 100.0336,\n",
       "         99.9174,  99.8854],\n",
       "       [ 99.9994, 100.0009, 100.0158, 100.0106,  99.9915, 100.0141,\n",
       "         99.9861, 100.0176,  99.9609, 100.0218,  99.9888,  99.9595,\n",
       "         99.9889,  99.9918],\n",
       "       [100.0002, 100.0001, 100.    ,  99.9998, 100.0004,  99.9992,\n",
       "        100.0005,  99.9967, 100.0039,  99.9985, 100.0019, 100.0007,\n",
       "        100.    , 100.0011],\n",
       "       [100.0524,  99.996 ,  99.9044,  99.9994, 100.15  ,  99.9438,\n",
       "        100.0165,  99.8536,  99.9511, 100.0112, 100.1533, 100.3446,\n",
       "         99.9703, 100.0489],\n",
       "       [ 99.9997,  99.9995, 100.0023,  99.9998,  99.995 , 100.0025,\n",
       "         99.9985, 100.0048, 100.0028,  99.9995,  99.9957,  99.9897,\n",
       "        100.0018,  99.9975],\n",
       "       [ 99.9993,  99.9992,  99.9988,  99.9999, 100.0007, 100.    ,\n",
       "         99.9991, 100.0014, 100.0003, 100.0023,  99.9994,  99.9981,\n",
       "        100.0004, 100.0003],\n",
       "       [ 99.9977, 100.0002,  99.9933,  99.9996, 100.0126,  99.9965,\n",
       "        100.0026,  99.9863,  99.9943, 100.0001, 100.0144, 100.0314,\n",
       "         99.9974, 100.0053]])"
      ]
     },
     "execution_count": 844,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7615941559557649"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=1\n",
    "(np.exp(x) - np.exp(-x))/(np.exp(-x) + np.exp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6, 6, 1, 0],\n",
       "       [3, 1, 3, 6],\n",
       "       [7, 4, 6, 3]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randint(0, 10, (3,4))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 5])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = np.array([1,2,3, 5])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7,  8,  4,  5],\n",
       "       [ 4,  3,  6, 11],\n",
       "       [ 8,  6,  9,  8]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a *= 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8, 18, 12, 20])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 3, 5, 1, 1, 1, 7, 5, 2],\n",
       "        [8, 1, 3, 6, 0, 2, 1, 0, 0],\n",
       "        [9, 2, 2, 2, 3, 0, 6, 5, 5],\n",
       "        [4, 9, 5, 4, 6, 1, 4, 9, 3]],\n",
       "\n",
       "       [[6, 9, 1, 8, 0, 0, 2, 2, 8],\n",
       "        [7, 2, 3, 0, 7, 8, 8, 3, 6],\n",
       "        [2, 5, 2, 0, 4, 1, 9, 3, 7],\n",
       "        [6, 1, 3, 1, 8, 2, 3, 1, 2]],\n",
       "\n",
       "       [[3, 2, 8, 3, 3, 1, 7, 0, 0],\n",
       "        [8, 2, 1, 1, 7, 8, 9, 4, 7],\n",
       "        [6, 5, 8, 6, 0, 8, 5, 3, 3],\n",
       "        [0, 0, 6, 0, 8, 6, 2, 2, 1]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO = np.random.randint(0,10, (3, 4, 9))\n",
    "dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "dO = np.concatenate([np.zeros((dO.shape[0], 1, dO.shape[2])), dO], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 9)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.zeros((dO.shape[0], 1, dO.shape[2])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 3., 5., 1., 1., 1., 7., 5., 2.],\n",
       "        [8., 1., 3., 6., 0., 2., 1., 0., 0.],\n",
       "        [9., 2., 2., 2., 3., 0., 6., 5., 5.],\n",
       "        [4., 9., 5., 4., 6., 1., 4., 9., 3.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [6., 9., 1., 8., 0., 0., 2., 2., 8.],\n",
       "        [7., 2., 3., 0., 7., 8., 8., 3., 6.],\n",
       "        [2., 5., 2., 0., 4., 1., 9., 3., 7.],\n",
       "        [6., 1., 3., 1., 8., 2., 3., 1., 2.]],\n",
       "\n",
       "       [[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [3., 2., 8., 3., 3., 1., 7., 0., 0.],\n",
       "        [8., 2., 1., 1., 7., 8., 9., 4., 7.],\n",
       "        [6., 5., 8., 6., 0., 8., 5., 3., 3.],\n",
       "        [0., 0., 6., 0., 8., 6., 2., 2., 1.]]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, 12, 15,  4,  1, 19, 14, 16],\n",
       "       [15,  0, 18,  6, 10, 13,  4,  5],\n",
       "       [ 9,  3, 18,  7,  1, 16,  0,  5],\n",
       "       [11,  7,  4, 13,  6,  1, 10,  3],\n",
       "       [ 6,  2,  1, 16,  9, 17,  1,  2],\n",
       "       [10,  0, 10, 14, 12,  2, 16, 10],\n",
       "       [ 9,  0,  5, 13, 18, 10,  5,  7],\n",
       "       [ 9, 19,  0,  0,  3, 15, 15, 12],\n",
       "       [12,  5, 10,  7,  0, 16, 13,  8],\n",
       "       [15, 16, 11,  2, 14, 15, 13,  2]])"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = np.random.randint(0,20, (10, 8))\n",
    "emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 7],\n",
       "       [4, 3, 6, 6],\n",
       "       [4, 1, 7, 5]])"
      ]
     },
     "execution_count": 546,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2,3, 7], [4,3,6,6], [4,1,7,5]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0])"
      ]
     },
     "execution_count": 548,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X == 6).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cross index must be 1 dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/anupbhutada/Documents/Courses/Neural Networks/lstm/neural_network.ipynb Cell 25\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/anupbhutada/Documents/Courses/Neural%20Networks/lstm/neural_network.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m emb[np\u001b[39m.\u001b[39;49mix_(np\u001b[39m.\u001b[39;49marange(\u001b[39m10\u001b[39;49m), X[:])]\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mix_\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/numpy/lib/index_tricks.py:102\u001b[0m, in \u001b[0;36mix_\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    100\u001b[0m         new \u001b[39m=\u001b[39m new\u001b[39m.\u001b[39mastype(_nx\u001b[39m.\u001b[39mintp)\n\u001b[1;32m    101\u001b[0m \u001b[39mif\u001b[39;00m new\u001b[39m.\u001b[39mndim \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m--> 102\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCross index must be 1 dimensional\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    103\u001b[0m \u001b[39mif\u001b[39;00m issubdtype(new\u001b[39m.\u001b[39mdtype, _nx\u001b[39m.\u001b[39mbool_):\n\u001b[1;32m    104\u001b[0m     new, \u001b[39m=\u001b[39m new\u001b[39m.\u001b[39mnonzero()\n",
      "\u001b[0;31mValueError\u001b[0m: Cross index must be 1 dimensional"
     ]
    }
   ],
   "source": [
    "emb[np.ix_(np.arange(10))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [15, 18, 18,  4,  1, 10,  5,  0, 10, 11],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2],\n",
       "        [19, 13, 16,  1, 17,  2, 10, 15, 16, 15]]])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.take(emb.T, X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 538,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N, T, emb_size \n",
    "dW = np.zeros_like(emb)\n",
    "dW\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       "\n",
       "       [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]],\n",
       "\n",
       "       [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
       "        [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]])"
      ]
     },
     "execution_count": 539,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO = np.tile(np.arange(10), (3,4,1))\n",
    "dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW.T[X, :] += dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 1, 1, 1, 1, 1, 1],\n",
       "       [0, 2, 2, 2, 2, 2, 2, 2],\n",
       "       [0, 3, 3, 3, 3, 3, 3, 3],\n",
       "       [0, 4, 4, 4, 4, 4, 4, 4],\n",
       "       [0, 5, 5, 5, 5, 5, 5, 5],\n",
       "       [0, 6, 6, 6, 6, 6, 6, 6],\n",
       "       [0, 7, 7, 7, 7, 7, 7, 7],\n",
       "       [0, 8, 8, 8, 8, 8, 8, 8],\n",
       "       [0, 9, 9, 9, 9, 9, 9, 9]])"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.add.at(dW.T, np.s_[X, :], dO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0,  0,  0,  0,  0,  0],\n",
       "       [ 0,  2,  1,  2,  2,  1,  2,  2],\n",
       "       [ 0,  4,  2,  4,  4,  2,  4,  4],\n",
       "       [ 0,  6,  3,  6,  6,  3,  6,  6],\n",
       "       [ 0,  8,  4,  8,  8,  4,  8,  8],\n",
       "       [ 0, 10,  5, 10, 10,  5, 10, 10],\n",
       "       [ 0, 12,  6, 12, 12,  6, 12, 12],\n",
       "       [ 0, 14,  7, 14, 14,  7, 14, 14],\n",
       "       [ 0, 16,  8, 16, 16,  8, 16, 16],\n",
       "       [ 0, 18,  9, 18, 18,  9, 18, 18]])"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.take(emb.T, X, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [15, 18, 18,  4,  1, 10,  5,  0, 10, 11],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2],\n",
       "        [19, 13, 16,  1, 17,  2, 10, 15, 16, 15]]])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False,  True,  True],\n",
       "       [False, False, False,  True],\n",
       "       [False,  True,  True,  True]])"
      ]
     },
     "execution_count": 492,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.arange(X.shape[1]) > np.array([1,2,0])[:, np.newaxis]\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[mask, :] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [15, 18, 18,  4,  1, 10,  5,  0, 10, 11],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [ 4,  6,  7, 13, 16, 14, 13,  0,  7,  2],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13],\n",
       "        [14,  4,  0, 10,  1, 16,  5, 15, 13, 13]],\n",
       "\n",
       "       [[ 1, 10,  1,  6,  9, 12, 18,  3,  0, 14],\n",
       "        [12,  0,  3,  7,  2,  0,  0, 19,  5, 16],\n",
       "        [16,  5,  5,  3,  2, 10,  7, 12,  8,  2],\n",
       "        [19, 13, 16,  1, 17,  2, 10, 15, 16, 15]]])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = np.array([1,2,0])\n",
    "dO_ = np.zeros_like(X)\n",
    "dO_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  7, 14, 19, 16, 15,  1,  4,  4, 19],\n",
       "       [ 5, 10, 13,  2, 15, 10,  9,  6, 16,  1],\n",
       "       [16,  4, 13, 19,  1,  1,  7,  1,  7, 14]])"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO = X[np.arange(3), 1, :]\n",
    "dO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 4,  7, 14, 19, 16, 15,  1,  4,  4, 19],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 5, 10, 13,  2, 15, 10,  9,  6, 16,  1],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n",
       "\n",
       "       [[16,  4, 13, 19,  1,  1,  7,  1,  7, 14],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
       "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0]]])"
      ]
     },
     "execution_count": 491,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dO_[np.arange(X.shape[0]), mask, :] = dO\n",
    "dO_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 6,  0,  9, 11,  2,  9, 17, 16,  6,  4],\n",
       "       [13, 19, 11,  0, 14, 16,  1, 15,  4,  9],\n",
       "       [13, 19, 11,  0, 14, 16,  1, 15,  4,  9],\n",
       "       [11,  1,  7, 17, 14,  2, 12, 11,  8, 15],\n",
       "       [18, 12, 11, 16, 16,  2, 13,  0, 15, 12],\n",
       "       [ 6,  0,  9, 11,  2,  9, 17, 16,  6,  4]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2d = X.reshape(-1, X.shape[2])\n",
    "X_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6,  0,  9, 11,  2,  9, 17, 16,  6,  4],\n",
       "        [13, 19, 11,  0, 14, 16,  1, 15,  4,  9]],\n",
       "\n",
       "       [[13, 19, 11,  0, 14, 16,  1, 15,  4,  9],\n",
       "        [11,  1,  7, 17, 14,  2, 12, 11,  8, 15]],\n",
       "\n",
       "       [[18, 12, 11, 16, 16,  2, 13,  0, 15, 12],\n",
       "        [ 6,  0,  9, 11,  2,  9, 17, 16,  6,  4]]])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_2d.reshape(*a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros_like(X_2d.reshape(-1, 2, 10))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = a.reshape(-1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(con):\n",
    "    return 1 if con else 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(1).repeat(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 885,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou = np.arange(9)\n",
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "metadata": {},
   "outputs": [],
   "source": [
    "bu = ou\n",
    "ou = ou[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 888,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
