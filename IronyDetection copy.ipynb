{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programming Assignment (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will solve an irony detection task: given a tweet, your job is to classify whether it is ironic or not.\n",
    "\n",
    "You will implement a new classifier that does not rely on feature engineering as in previous homeworks. Instead, you will use pretrained word embeddings downloaded from using the `irony.py` script as your input feature vectors. Then, you will encode your sequence of word embeddings with an (already implemented) LSTM and classify based on its final hidden state.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is so that you don't have to restart the kernel everytime you edit hmm.py\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We will use the dataset from SemEval-2018: https://github.com/Cyvhee/SemEval2018-Task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from irony import load_datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_sentences, train_labels, test_sentences, test_labels, label2i = load_datasets()\n",
    "\n",
    "# TODO: Split train into train/dev\n",
    "train_sentences, dev_sentences, train_labels, dev_labels = train_test_split(train_sentences, train_labels, train_size=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline: Naive Bayes\n",
    "\n",
    "We have provided the solution for the Naive Bayes part from HW2 in [bayes.py](bayes.py)\n",
    "\n",
    "There are two implementations: NaiveBayesHW2 is what was expected from HW2. However, we will use a more effecient implementation of it that uses vector operations to calculate the probabilities. Please go through it if you would like to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Vectorizing Text: 100%|██████████| 3834/3834 [00:00<00:00, 23689.65it/s]\n",
      "Vectorizing Text: 100%|██████████| 3834/3834 [00:00<00:00, 24534.94it/s]\n",
      "Vectorizing Text: 100%|██████████| 784/784 [00:00<00:00, 28879.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: Naive Bayes Classifier\n",
      "F1-score Ironic: 0.6402966625463535\n",
      "Avg F1-score: 0.6284487265300938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from irony import run_nb_baseline\n",
    "\n",
    "run_nb_baseline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement avg_f1_score() in [util.py](util.py). Then re-run the above cell  (2 Points)\n",
    "\n",
    "So the micro F1-score for the test set of the Ironic Class using a Naive Bayes Classifier is **0.64**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Word2Vec  (Total: 18 Points)\n",
    "\n",
    "Unlike sentiment, Irony is very subjective, and there is no word list for ironic and non-ironic tweets. This makes hand-engineering features tedious, therefore, we will use word embeddings as input to the classifier, and make the model automatically extract features aka learn weights for the embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer for Tweets\n",
    "\n",
    "\n",
    "Tweets are very different from normal document text. They have emojis, hashtags and bunch of other special character. Therefore, we need to create a suitable tokenizer for this kind of text.\n",
    "\n",
    "Additionally, as described in class, we also need to have a consistent input length of the text document in order for the neural networks built over it to work correctly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Create a Tokenizer with Padding (5 Points)\n",
    "\n",
    "Our Tokenizer class is meant for tokenizing and padding batches of inputs. This is done\n",
    "before we encode text sequences as torch Tensors.\n",
    "\n",
    "Update the following class by completing the todo statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Optional, Tuple\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import spacy\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Tokenizes and pads a batch of input sentences.\"\"\"\n",
    "\n",
    "    def __init__(self, pad_symbol: Optional[str] = \"<PAD>\"):\n",
    "        \"\"\"Initializes the tokenizer\n",
    "\n",
    "        Args:\n",
    "            pad_symbol (Optional[str], optional): The symbol for a pad. Defaults to \"<PAD>\".\n",
    "        \"\"\"\n",
    "        self.pad_symbol = pad_symbol\n",
    "        self.nlp = spacy.load(\"en_core_web_lg\")\n",
    "    \n",
    "    def __call__(self, batch: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes each sentence in the batch, and pads them if necessary so\n",
    "        that we have equal length sentences in the batch.\n",
    "\n",
    "        Args:\n",
    "            batch (List[str]): A List of sentence strings\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: A List of equal-length token Lists.\n",
    "        \"\"\"\n",
    "        batch = self.tokenize(batch)\n",
    "        batch = self.pad(batch)\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def tokenize(self, sentences: List[str]) -> List[List[str]]:\n",
    "        \"\"\"Tokenizes the List of string sentences into a Lists of tokens using spacy tokenizer.\n",
    "\n",
    "        Args:\n",
    "            sentences (List[str]): The input sentence.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: The tokenized version of the sentence.\n",
    "        \"\"\"\n",
    "        tokenized_sents = []\n",
    "        # TODO: Tokenize the input with spacy.\n",
    "        for sent in sentences:\n",
    "            sent_tokens = [token.text.lower() for token in self.nlp(sent)]\n",
    "        # TODO: Make sure the start token is the special <SOS> token and the end token\n",
    "        #       is the special <EOS> token\n",
    "            sent_tokens = ['<SOS>'] + sent_tokens + ['<EOS>']\n",
    "            tokenized_sents.append(sent_tokens)\n",
    "\n",
    "        return tokenized_sents\n",
    "        # raise NotImplementedError\n",
    "\n",
    "    def pad(self, batch: List[List[str]]) -> List[List[str]]:\n",
    "        \"\"\"Appends pad symbols to each tokenized sentence in the batch such that\n",
    "        every List of tokens is the same length. This means that the max length sentence\n",
    "        will not be padded.\n",
    "\n",
    "        Args:\n",
    "            batch (List[List[str]]): Batch of tokenized sentences.\n",
    "\n",
    "        Returns:\n",
    "            List[List[str]]: Batch of padded tokenized sentences. \n",
    "        \"\"\"\n",
    "        # TODO: For each sentence in the batch, append the special <P>\n",
    "        #       symbol to it n times to make all sentences equal length\n",
    "        out_batch = []\n",
    "        max_len = max([len(sent) for sent in batch])\n",
    "        for sent in batch:\n",
    "            out_batch.append(sent + [self.pad_symbol]*(max_len - len(sent)))\n",
    "        return out_batch\n",
    "        # raise NotImplementedError\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vocabulary of the dataset: use both training and test sets here\n",
    "\n",
    "SPECIAL_TOKENS = ['<UNK>', '<PAD>', '<SOS>', '<EOS>']\n",
    "\n",
    "all_data = train_sentences + dev_sentences + test_sentences\n",
    "my_tokenizer = Tokenizer()\n",
    "\n",
    "tokenized_data = my_tokenizer.tokenize(all_data)\n",
    "vocab = sorted(set([w for ws in tokenized_data + [SPECIAL_TOKENS] for w in ws]))\n",
    "\n",
    "with open('vocab.txt', 'w') as vf:\n",
    "    vf.write('\\n'.join(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "We use GloVe embeddings https://nlp.stanford.edu/projects/glove/. But these do not necessarily have all of the tokens that will occur in tweets! Hoad the GloVe embeddings, pruning them to only those words in vocab.txt. This is to reduce the memory and runtime of your model.\n",
    "\n",
    "Then, find the out-of-vocabulary words (oov) and add them to the encoding dictionary and the embeddings matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dowload the gloVe vectors for Twitter tweets. This will download a file called glove.twitter.27B.zip\n",
    "# ! wget https://nlp.stanford.edu/data/glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip glove.twitter.27B.zip\n",
    "# if there is an error, please download the zip file again\n",
    "\n",
    "# ! unzip glove.twitter.27B.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see what files are there:\n",
    "\n",
    "! ls . | grep \"glove.*.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For this assignment, we will use glove.twitter.27B.50d.txt which has 50 dimensional word vectors\n",
    "# Feel free to experiment with vectors of other sizes\n",
    "\n",
    "embedding_path = '/Users/anupbhutada/Documents/Courses/Natural Langauge Processing/Assignment4/assignment_4/glove.twitter.27B.100d.txt'\n",
    "vocab_path = \"./vocab.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a custom Embedding Layer\n",
    "\n",
    "Now the GloVe file has vectors for about 1.2 million words. However, we only need the vectors for a very tiny fraction of words -> the unique words that are there in the classification corpus. Some of the next tasks will be to create a custom embedding layer that has the vectors for this small set of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Extracting word vectors from GloVe (3 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Tuple\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def read_pretrained_embeddings(\n",
    "    embeddings_path: str,\n",
    "    vocab_path: str\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    \"\"\"Read the embeddings matrix and make a dict hashing each word.\n",
    "\n",
    "    Note that we have provided the entire vocab for train and test, so that for practical purposes\n",
    "    we can simply load those words in the vocab, rather than all 27B embeddings\n",
    "\n",
    "    Args:\n",
    "        embeddings_path (str): _description_\n",
    "        vocab_path (str): _description_\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Dict[str, int], torch.FloatTensor]: _description_\n",
    "    \"\"\"\n",
    "    word2i = {}\n",
    "    vectors = []\n",
    "    \n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()]) \n",
    "    \n",
    "    print(f\"Reading embeddings from {embeddings_path}...\")\n",
    "    with open(embeddings_path, \"r\") as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            word, *weights = line.rstrip().split(\" \")\n",
    "            # TODO: Build word2i and vectors such that\n",
    "            #       each word points to the index of its vector,\n",
    "            #       and only words that exist in `vocab` are in our embeddings\n",
    "            if word in vocab:\n",
    "                word2i.update({word: i})\n",
    "                vectors.append(weights)\n",
    "                i += 1\n",
    "            # raise NotImplementedError\n",
    "\n",
    "    return (word2i, torch.tensor(np.array(vectors, dtype=np.float32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Get GloVe Out of Vocabulary (oov) words (0 Points)\n",
    "\n",
    "The task is to find the words in the Irony corpus that are not in the GloVe Word list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oovs(vocab_path: str, word2i: Dict[str, int]) -> List[str]:\n",
    "    \"\"\"Find the vocab items that do not exist in the glove embeddings (in word2i).\n",
    "    Return the List of such (unique) words.\n",
    "\n",
    "    Args:\n",
    "        vocab_path: List of batches of sentences.\n",
    "        word2i (Dict[str, int]): _description_\n",
    "\n",
    "    Returns:\n",
    "        List[str]: _description_\n",
    "    \"\"\"\n",
    "    with open(vocab_path, encoding='utf8') as vf:\n",
    "        vocab = set([w.strip() for w in vf.readlines()])\n",
    "    \n",
    "    glove_and_vocab = set(word2i.keys())\n",
    "    vocab_and_not_glove = vocab - glove_and_vocab\n",
    "    return list(vocab_and_not_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Update the embeddings with oov words (3 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_new_embedding_weights(num_embeddings: int, dim: int) -> torch.FloatTensor:\n",
    "    \"\"\"xavier initialization for the embeddings of words in train, but not in gLove.\n",
    "\n",
    "    Args:\n",
    "        num_embeddings (int): _description_\n",
    "        dim (int): _description_\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: _description_\n",
    "    \"\"\"\n",
    "    # TODO: Initialize a num_embeddings x dim matrix with xiavier initiialization\n",
    "    #      That is, a normal distribution with mean 0 and standard deviation of dim^-0.5\n",
    "    oov_emb = torch.empty(num_embeddings, dim)\n",
    "    torch.nn.init.xavier_normal_(oov_emb)\n",
    "    return oov_emb\n",
    "    # raise NotImplementedError\n",
    "\n",
    "\n",
    "def update_embeddings(\n",
    "    glove_word2i: Dict[str, int],\n",
    "    glove_embeddings: torch.FloatTensor,\n",
    "    oovs: List[str]\n",
    ") -> Tuple[Dict[str, int], torch.FloatTensor]:\n",
    "    # TODO: Add the oov words to the dict, assigning a new index to each\n",
    "    max_ind = max(glove_word2i.values())\n",
    "    glove_word2i.update({oov: new_ind for oov, new_ind in zip(oovs, range(max_ind + 1, max_ind + 1 + len(oovs)))})\n",
    "    # TODO: Concatenate a new row to embeddings for each oov\n",
    "    #       initialize those new rows with `intialize_new_embedding_weights`\n",
    "    new_glove_embeddings = torch.vstack((glove_embeddings, intialize_new_embedding_weights(len(oovs), glove_embeddings.shape[1])))\n",
    "    # TODO: Return the tuple of the dictionary and the new embeddings matrix\n",
    "    return glove_word2i, new_glove_embeddings\n",
    "    # raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading embeddings from /Users/anupbhutada/Documents/Courses/Natural Langauge Processing/Assignment4/assignment_4/glove.twitter.27B.100d.txt...\n"
     ]
    }
   ],
   "source": [
    "def make_batches(sequences: List[str], batch_size: int) -> List[List[str]]:\n",
    "    \"\"\"Yield batch_size chunks from sequences.\"\"\"\n",
    "    # TODO\n",
    "    for i in range(0, len(sequences), batch_size):\n",
    "        yield sequences[i: i + batch_size]\n",
    "\n",
    "\n",
    "# TODO: Set your preferred batch size\n",
    "batch_size = 16\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "# We make batches now and use those.\n",
    "batch_tokenized = []\n",
    "# Note: Labels need to be batched in the same way to ensure\n",
    "# We have train sentence and label batches lining up.\n",
    "for batch in make_batches(train_sentences, batch_size):\n",
    "    batch_tokenized.append(tokenizer(batch))\n",
    "\n",
    "\n",
    "glove_word2i, glove_embeddings = read_pretrained_embeddings(\n",
    "    embedding_path,\n",
    "    vocab_path\n",
    ")\n",
    "\n",
    "# Find the out-of-vocabularies\n",
    "oovs = get_oovs(vocab_path, glove_word2i)\n",
    "\n",
    "# Add the oovs from training data to the word2i encoding, and as new rows\n",
    "# to the embeddings matrix\n",
    "word2i, embeddings = update_embeddings(glove_word2i, glove_embeddings, oovs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6002, 15299)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(oovs), len(word2i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding words to integers: DO NOT EDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use these functions to encode your batches before you call the train loop.\n",
    "\n",
    "def encode_sentences(batch: List[List[str]], word2i: Dict[str, int]) -> torch.LongTensor:\n",
    "    \"\"\"Encode the tokens in each sentence in the batch with a dictionary\n",
    "\n",
    "    Args:\n",
    "        batch (List[List[str]]): The padded and tokenized batch of sentences.\n",
    "        word2i (Dict[str, int]): The encoding dictionary.\n",
    "\n",
    "    Returns:\n",
    "        torch.LongTensor: The tensor of encoded sentences.\n",
    "    \"\"\"\n",
    "    UNK_IDX = word2i[\"<UNK>\"]\n",
    "    tensors = []\n",
    "    for sent in batch:\n",
    "        tensors.append(torch.LongTensor([word2i.get(w, UNK_IDX) for w in sent]))\n",
    "        \n",
    "    return torch.stack(tensors)\n",
    "\n",
    "\n",
    "def encode_labels(labels: List[int]) -> torch.FloatTensor:\n",
    "    \"\"\"Turns the batch of labels into a tensor\n",
    "\n",
    "    Args:\n",
    "        labels (List[int]): List of all labels in the batch\n",
    "\n",
    "    Returns:\n",
    "        torch.FloatTensor: Tensor of all labels in the batch\n",
    "    \"\"\"\n",
    "    return torch.LongTensor([int(l) for l in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling   ( 7 Points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "# Notice there is a single TODO in the model\n",
    "class IronyDetector(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim: int,\n",
    "        hidden_dim: int,\n",
    "        embeddings_tensor: torch.FloatTensor,\n",
    "        pad_idx: int,\n",
    "        output_size: int,\n",
    "        dropout_val: float = 0.3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.pad_idx = pad_idx\n",
    "        self.dropout_val = dropout_val\n",
    "        self.output_size = output_size\n",
    "        # TODO: Initialize the embeddings from the weights matrix.\n",
    "        #       Check the documentation for how to initialize an embedding layer\n",
    "        #       from a pretrained embedding matrix. \n",
    "        #       Be careful to set the `freeze` parameter!\n",
    "        #       Docs are here: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html#torch.nn.Embedding.from_pretrained\n",
    "        self.embeddings = torch.nn.Embedding.from_pretrained(embeddings_tensor, freeze=True)\n",
    "        # Dropout regularization\n",
    "        # https://jmlr.org/papers/v15/srivastava14a.html\n",
    "        self.dropout_layer = torch.nn.Dropout(p=self.dropout_val, inplace=False)\n",
    "        # Bidirectional 2-layer LSTM. Feel free to try different parameters.\n",
    "        # https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            self.input_dim,\n",
    "            self.hidden_dim,\n",
    "            num_layers=2,\n",
    "            dropout=dropout_val,\n",
    "            batch_first=True,\n",
    "            bidirectional=False,\n",
    "        )\n",
    "        # For classification over the final LSTM state.\n",
    "        # self.classifier = torch.nn.Linear(hidden_dim*2, self.output_size)\n",
    "        self.classifier = torch.nn.Linear(hidden_dim, self.output_size)\n",
    "        # self.classifier = torch.nn.Linear(hidden_dim*2, 8)\n",
    "        # self.relu = torch.nn.ReLU()\n",
    "        # self.classifier2 = torch.nn.Linear(8, self.output_size)\n",
    "        self.log_softmax = torch.nn.LogSoftmax(dim=2)\n",
    "    \n",
    "    def encode_text(\n",
    "        self,\n",
    "        symbols: torch.Tensor\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"Encode the (batch of) sequence(s) of token symbols with an LSTM.\n",
    "            Then, get the last (non-padded) hidden state for each symbol and return that.\n",
    "\n",
    "        Args:\n",
    "            symbols (torch.Tensor): The batch size x sequence length tensor of input tokens\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The final hiddens tate of the LSTM, which represents an encoding of\n",
    "                the entire sentence\n",
    "        \"\"\"\n",
    "        # First we get the embedding for each input symbol\n",
    "        embedded = self.embeddings(symbols)\n",
    "        embedded = self.dropout_layer(embedded)\n",
    "        # Packs embedded source symbols into a PackedSequence.\n",
    "        # This is an optimization when using padded sequences with an LSTM\n",
    "        lens = (symbols != self.pad_idx).sum(dim=1).to(\"cpu\")\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lens, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        # -> batch_size x seq_len x encoder_dim, (h0, c0).\n",
    "        packed_outs, (H, C) = self.lstm(packed)\n",
    "        encoded, _ = torch.nn.utils.rnn.pad_packed_sequence(\n",
    "            packed_outs,\n",
    "            batch_first=True,\n",
    "            padding_value=self.pad_idx,\n",
    "            total_length=None,\n",
    "        )\n",
    "        # Now we have the representation of eahc token encoded by the LSTM.\n",
    "        # encoded, (H, C) = self.lstm(embedded)\n",
    "        \n",
    "        # This part looks tricky. All we are doing is getting a tensor\n",
    "        # That indexes the last non-PAD position in each tensor in the batch.\n",
    "        last_enc_out_idxs = lens - 1\n",
    "        # -> B x 1 x 1.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.view([encoded.size(0)] + [1, 1])\n",
    "        # -> 1 x 1 x encoder_dim. This indexes the last non-padded dimension.\n",
    "        last_enc_out_idxs = last_enc_out_idxs.expand(\n",
    "            [-1, -1, encoded.size(-1)]\n",
    "        )\n",
    "        # Get the final hidden state in the LSTM\n",
    "        last_hidden = torch.gather(encoded, 1, last_enc_out_idxs)\n",
    "        return last_hidden\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        symbols: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "        encoded_sents = self.encode_text(symbols)\n",
    "        output = self.classifier(encoded_sents)\n",
    "        # output = self.relu(output)\n",
    "        # output = self.classifier2(output)\n",
    "        return self.log_softmax(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model: torch.nn.Module, dev_sequences: List[torch.Tensor]):\n",
    "    preds = []\n",
    "    # TODO: Get the predictions for the dev_sequences using the model\n",
    "    pred_batches = []\n",
    "    eval_model = model.eval()\n",
    "    for dev_batch in dev_sequences:\n",
    "        pred_probs = eval_model(dev_batch).squeeze(1)\n",
    "        pred_labels = torch.argmax(pred_probs, axis=1)\n",
    "        pred_batches.append(pred_labels)\n",
    "    return torch.concat(pred_batches)\n",
    "    # raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import random\n",
    "from util import avg_f1_score, f1_score\n",
    "\n",
    "\n",
    "def training_loop(\n",
    "    num_epochs,\n",
    "    train_features,\n",
    "    train_labels,\n",
    "    dev_features,\n",
    "    dev_labels,\n",
    "    optimizer,\n",
    "    model,\n",
    "):\n",
    "    dev_labels = torch.concat(dev_labels)\n",
    "    print(\"Training...\")\n",
    "    loss_func = torch.nn.NLLLoss()\n",
    "    batches = list(zip(train_features, train_labels))\n",
    "    random.shuffle(batches)\n",
    "    \n",
    "    train_labels_all = torch.concat(list(zip(*batches))[1])\n",
    "    for i in range(num_epochs):\n",
    "        train_preds_all = []\n",
    "        losses = []\n",
    "        for features, labels in tqdm(batches):\n",
    "            # Empty the dynamic computation graph\n",
    "            optimizer.zero_grad()\n",
    "            preds = model(features).squeeze(1)\n",
    "            loss = loss_func(preds, labels)\n",
    "            # Backpropogate the loss through our model\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            losses.append(loss.item())\n",
    "            train_preds_all.append(torch.argmax(preds, axis=1))\n",
    "        train_preds_all = torch.concat(train_preds_all)\n",
    "        \n",
    "        print(f\"epoch {i}, loss: {sum(losses)/len(losses)}\")\n",
    "        # Estimate the f1 score for the development set\n",
    "        print(\"Evaluating train...\")\n",
    "        train_f1 = f1_score(train_preds_all, train_labels_all, label2i['1'])\n",
    "        print(f\"Train F1 {train_f1}\")\n",
    "        print(\"Evaluating dev...\")\n",
    "        preds = predict(model, dev_features)\n",
    "        dev_f1 = f1_score(preds, dev_labels, label2i['1'])\n",
    "        dev_avg_f1 = f1_score(preds, dev_labels, list(label2i.keys()))\n",
    "        print(f\"Dev F1 {dev_f1}\")\n",
    "        print(f\"Avf Dev F1 {dev_f1}\")\n",
    "        \n",
    "    # Return the trained model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating batches...\n",
      "Encoding batches...\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the model and run the training loop \n",
    "#       on your train/dev splits. Set and tweak hyperparameters.\n",
    "batch_size = 32\n",
    "\n",
    "print (\"Creating batches...\")\n",
    "batch_train_tokenized = []\n",
    "batch_tokenized_train = [tokenizer(batch) for batch in make_batches(train_sentences, batch_size)]\n",
    "batch_labels_train = list(make_batches(train_labels, batch_size))\n",
    "batch_tokenized_dev = [tokenizer(batch) for batch in make_batches(dev_sentences, batch_size)]\n",
    "batch_labels_dev = list(make_batches(dev_labels, batch_size))\n",
    "\n",
    "print (\"Encoding batches...\")\n",
    "batch_sentences_train = [encode_sentences(batch, word2i) for batch in batch_tokenized_train]\n",
    "batch_labels_train = [encode_labels(batch_labels) for batch_labels in batch_labels_train]\n",
    "batch_sentences_dev = [encode_sentences(batch, word2i) for batch in batch_tokenized_dev]\n",
    "batch_labels_dev = [encode_labels(batch_labels) for batch_labels in batch_labels_dev]\n",
    "\n",
    "print (\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mr/j7cgrptd10344f4v1yr36nv00000gn/T/ipykernel_71025/573141400.py:26: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for features, labels in tqdm(batches):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "651ca98740c4484f916c5b1069896135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, loss: 0.6912174640844265\n",
      "Evaluating train...\n",
      "Train F1 0.5491905354919052\n",
      "Evaluating dev...\n",
      "Dev F1 0.6487093153759821\n",
      "Avf Dev F1 0.6487093153759821\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e17457d842d24e1a83ffebd39eca5a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, loss: 0.666224138190349\n",
      "Evaluating train...\n",
      "Train F1 0.5917678812415654\n",
      "Evaluating dev...\n",
      "Dev F1 0.6275033377837116\n",
      "Avf Dev F1 0.6275033377837116\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf27bb0350548919c04baf68f326898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2, loss: 0.6341770098855098\n",
      "Evaluating train...\n",
      "Train F1 0.6487351905219341\n",
      "Evaluating dev...\n",
      "Dev F1 0.6553524804177545\n",
      "Avf Dev F1 0.6553524804177545\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85c16587f664cdba23103ef19fb5e6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3, loss: 0.6138720260933042\n",
      "Evaluating train...\n",
      "Train F1 0.6662324975577989\n",
      "Evaluating dev...\n",
      "Dev F1 0.6546854942233633\n",
      "Avf Dev F1 0.6546854942233633\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb282bbd98423d81538dcfce5b2202",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4, loss: 0.5981657020747662\n",
      "Evaluating train...\n",
      "Train F1 0.6791093647675179\n",
      "Evaluating dev...\n",
      "Dev F1 0.6573957016434893\n",
      "Avf Dev F1 0.6573957016434893\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa9f8f4a75ad47ee90c2864c4d87cedb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5, loss: 0.5840278367201487\n",
      "Evaluating train...\n",
      "Train F1 0.6924083769633508\n",
      "Evaluating dev...\n",
      "Dev F1 0.6633039092055486\n",
      "Avf Dev F1 0.6633039092055486\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ceb512c4370f4335944f67f3e769e6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6, loss: 0.5702624889090657\n",
      "Evaluating train...\n",
      "Train F1 0.7040314650934121\n",
      "Evaluating dev...\n",
      "Dev F1 0.67\n",
      "Avf Dev F1 0.67\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa1c1e931787445faa2d143a37451ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7, loss: 0.5561489127576351\n",
      "Evaluating train...\n",
      "Train F1 0.7170059093893631\n",
      "Evaluating dev...\n",
      "Dev F1 0.6716981132075471\n",
      "Avf Dev F1 0.6716981132075471\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b4ff3308ec4705b0569fbb1fc6d594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8, loss: 0.5406519587462147\n",
      "Evaluating train...\n",
      "Train F1 0.7267939433838051\n",
      "Evaluating dev...\n",
      "Dev F1 0.6750948166877371\n",
      "Avf Dev F1 0.6750948166877371\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd812eb738634ddd9de3917dff150057",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9, loss: 0.5237347089375058\n",
      "Evaluating train...\n",
      "Train F1 0.7478660538411032\n",
      "Evaluating dev...\n",
      "Dev F1 0.6760204081632653\n",
      "Avf Dev F1 0.6760204081632653\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1fe31dccc8f4154927b65cf16765a8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10, loss: 0.5059003544350466\n",
      "Evaluating train...\n",
      "Train F1 0.7608766764802094\n",
      "Evaluating dev...\n",
      "Dev F1 0.6735483870967741\n",
      "Avf Dev F1 0.6735483870967741\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fd5228658dc4bad94cbdad859fbc748",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11, loss: 0.4868439494942625\n",
      "Evaluating train...\n",
      "Train F1 0.7730380983393033\n",
      "Evaluating dev...\n",
      "Dev F1 0.6613333333333333\n",
      "Avf Dev F1 0.6613333333333333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a832eaa7962141d4a4fdda3ff7333179",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12, loss: 0.46577044172833365\n",
      "Evaluating train...\n",
      "Train F1 0.7868532378782948\n",
      "Evaluating dev...\n",
      "Dev F1 0.6720214190093708\n",
      "Avf Dev F1 0.6720214190093708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94909452dfe242ef88602df352b9fc70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13, loss: 0.4430972963261108\n",
      "Evaluating train...\n",
      "Train F1 0.8045454545454546\n",
      "Evaluating dev...\n",
      "Dev F1 0.6729222520107239\n",
      "Avf Dev F1 0.6729222520107239\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "247b407c95014dfaa953943dcd03cf19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14, loss: 0.41695345457022387\n",
      "Evaluating train...\n",
      "Train F1 0.8229200388475235\n",
      "Evaluating dev...\n",
      "Dev F1 0.674731182795699\n",
      "Avf Dev F1 0.674731182795699\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb50d5286304533baeb07eb8ec0cb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15, loss: 0.39362564620872337\n",
      "Evaluating train...\n",
      "Train F1 0.8376288659793815\n",
      "Evaluating dev...\n",
      "Dev F1 0.6720214190093708\n",
      "Avf Dev F1 0.6720214190093708\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13524a114e204d1f9f6312e7653b5e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16, loss: 0.36977895225087803\n",
      "Evaluating train...\n",
      "Train F1 0.8519948519948519\n",
      "Evaluating dev...\n",
      "Dev F1 0.6973848069738481\n",
      "Avf Dev F1 0.6973848069738481\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21c293f5899403b8a476e1acf937eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17, loss: 0.3518550934580465\n",
      "Evaluating train...\n",
      "Train F1 0.8634900193174501\n",
      "Evaluating dev...\n",
      "Dev F1 0.7033997655334114\n",
      "Avf Dev F1 0.7033997655334114\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0d97f9541c4276bedac0bb8737aa8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18, loss: 0.3503439455914001\n",
      "Evaluating train...\n",
      "Train F1 0.8628589867699259\n",
      "Evaluating dev...\n",
      "Dev F1 0.6981818181818182\n",
      "Avf Dev F1 0.6981818181818182\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec4d0352223439ea8f41acb9f9d69cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/96 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19, loss: 0.3543873069187005\n",
      "Evaluating train...\n",
      "Train F1 0.857234726688103\n",
      "Evaluating dev...\n",
      "Dev F1 0.7033997655334114\n",
      "Avf Dev F1 0.7033997655334114\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "LR = 0.0005\n",
    "\n",
    "model = IronyDetector(input_dim=100,\n",
    "                    hidden_dim=32,\n",
    "                    embeddings_tensor=embeddings,\n",
    "                    pad_idx=word2i['<PAD>'],\n",
    "                    output_size=2,\n",
    "                    dropout_val=0.)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), LR)\n",
    "\n",
    "model = training_loop(\n",
    "    num_epochs,\n",
    "    batch_sentences_train,\n",
    "    batch_labels_train,\n",
    "    batch_sentences_dev,\n",
    "    batch_labels_dev,\n",
    "    optimizer,\n",
    "    model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.8691701998788612\n",
      "acc: 0.8591457450277143\n"
     ]
    }
   ],
   "source": [
    "train_preds = predict(model, batch_sentences_train)\n",
    "print ('f1 score:', f1_score(train_preds, torch.concat(batch_labels_train)))\n",
    "print ('acc:', np.array(train_preds==torch.concat(batch_labels_train)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.7033997655334114\n",
      "acc: 0.6701434159061278\n"
     ]
    }
   ],
   "source": [
    "dev_preds = predict(model, batch_sentences_dev)\n",
    "print ('f1 score:', f1_score(dev_preds, torch.concat(batch_labels_dev)))\n",
    "print ('acc:', np.array(dev_preds==torch.concat(batch_labels_dev)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_encoded = encode_sentences(tokenizer(test_sentences), word2i)\n",
    "test_labels_encoded = encode_labels(test_labels)\n",
    "# test_preds = torch.argmax(model.eval()(test_encoded).squeeze(1), axis=1)\n",
    "# print (f1_score(test_preds, test_labels_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score: 0.6171574903969271\n",
      "acc: 0.6186224489795918\n"
     ]
    }
   ],
   "source": [
    "test_preds = torch.argmax(model.eval()(test_encoded).squeeze(1), axis=1)\n",
    "print ('f1 score:', f1_score(test_preds, test_labels_encoded))\n",
    "print ('acc:', np.array(test_preds==test_labels_encoded).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x700 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# np.set_printoptions(precision=2, suppress=True)\n",
    "import seaborn as sns\n",
    "# plt.figure(figsize=(20,20))\n",
    "sns.set(rc={'figure.figsize':(7,7)})\n",
    "sns.heatmap(confusion_matrix(test_labels_encoded, test_preds), annot=True, annot_kws={\"size\": 15}, fmt='g')\n",
    "plt.ylabel('Actual',fontsize=15)\n",
    "plt.xlabel('Predictions',fontsize=15)\n",
    "plt.title('Confusion Matrix',fontsize=17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Written Assignment (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Describe what the task is, and how it could be useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task here is to detect the presence of irony in tweets. The problem is a binary classification task with presence of irony as class 1 and absence as 0. The task can be useful when trying to analyse sentiments realated to a topic on social media. The presence of irony can often change the meaning from a seemingly positive sentiment to a negative one. If a model fails to detect this aspect, the results of the analysis can be misleading. Adding a irony detection model to the pipeline can help add more information and the analysis can be made while being aware of this aspect being present in the model. A sentiment classification model may overlook this detail as it may not be trained on a dataset with many instances of such irony. There trining a model explicitly to detect irony can be very useful to understand the direction in which the sentiments are pointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Describe, at the high level, that is, without mathematical rigor, how pretrained word embeddings like the ones we relied on here are computed. Your description can discuss the Word2Vec class of algorithms, GloVe, or a similar method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pretrained word embeddings, whether they come from word2vec of GloVe, fundamentally try to represent the meanings of words in low dimensional vectors (50-500 dimensions). These vectors have relative distances that can explain words occurring in similar contect and the relative directions correspond to meanings that make common sense. In short they create a vector space where similar words appear close to each other and moving in any particular direction changes the meanings of words that correspond a uniform aspect in the meaning (positive/negative for example). \n",
    "\n",
    "The pretrained embeddings in case of word2vec are computed using a predictive model. We either try to predict the target word given a context, incase of CBOW, or predict the context given a target word, in case of skip-gram method. While making this prediction, the models use trainable weights that produce a vector embeddings for every word and as the model gets better at making predictions, the embedings become better at encoding the relative meanings of words. \n",
    "\n",
    "In case of GLoVe, the embeddings are creating using a word co-occurrence matrix. Given a corpus, the word co-occurence matrix becomes a huge vocabulary x context matrix that counts how frequently a word appears in context of other words. This large matrix is then factorized to yeild a word x features matrix that contextualizes the menaing of words in the corpus into manageable ambeddings.\n",
    "\n",
    "Both these methods yeild word embeddings that roughly do the same job of creating meaingful vectors that can represent relative the meanings of words and can provide various models a more usefule representation of words as compared to the sparse representation of words computed using word frequencies of TFIDF values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. What are some of the benefits of using word embeddings instead of e.g. a bag of words?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bag of words can do a good job at contextualizing words, but these generally yeild very high-dimensional sparse vectors. Due to their nature, they can only capture high level information about the word based on the documents they appear in. When producing a vector representation for a document each word is represented by a position in the high dimensional matrix and if a word does not appear enough times in the corpus while trining for a specific task, the model does not know how to generalize to a new word given a new example. Embeddings solve this problem by creating low-dimensional dense representations of words that represent meanings in some vector space. Using such contexual embeddings, the model can understand a new word even if it hasnt appeared too often during training and therefore can lead to better performance across models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. What is the difference between Binary Cross Entropy loss and the negative log likelihood loss we used here (`torch.nn.NLLLoss`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the negative log lokelihood loss in here since we have the last layer in our model as the log softmax. The log softmax layer as opposed to the softmax layer computes the log of the probabilities for each class and therefore we cannot use the catgorical cross entropy loss that is typicallu used since this loss function also computes to log of probabilities while arriving at the loss. We, therefore, use the NLLLoss since this layer is aware that we already have the log probabilities computed and therefore just picks up the log probability corresponding the target label and negates it to compute the loss for that observation. This loss is then averaged over the batch to return the total loss for the batch and allow the model to backpropagate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Show your experimental results. Indicate any changes to hyperparameters, data splits, or architectural changes you made, and how those effected results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main problem with the model based on my experiments was that the model overfit very easily. Especially when the the freeze parameter is set to FALSE, the trining loss quickly reduces to powers of -4 and -5 with trainng F1 at 1 even with dropouts set to large values like 0.7 or 0.8. The reason for this may be that there are about 6k words out of a total vocabulary of about 15k words that are not in the GLoVe embeddings. This means that about 40% of the words have randomly initialized embeddings. And since the train and test sets are relatovely small, there may not be a large overlap between the words in train set and words in test set. As a result a large proportion of the randomly initialized embeddings are never trained. This results in a significant number of embedding vectors that are just randomly initialied vectors from the OOV tokens in the test data. And as a result the model is not able to perform on the test data since these word embeddings make no sense and add noise to the input data.\n",
    "\n",
    "One way to make this problem less severe would be to remove the OOV words appearing only in the test set from our vocabulary so that we do not initialize those tokens with random embeddings. This way we can route these unseen words to the <UNK> token which can represent them better than randomly initialized weights. Also to train the <UNK> token we may have to randomly selct a few tokens from the training set and delete them from the vocabulary, otherwise the model may never come across the <UNK> token while training. \n",
    "\n",
    "I found during my expriments (summarized in the table below), that opening up the embedding layer to be trainable results in severe overfitting as probably there is very little overlapping tokens in the training and testing set. I also found that using 200d embeddings causes too much overfitting as the training F1 quickly reached 1 with testing F1 struggling to cross the benchmark naive bayes performance. But one of the best results in my experiments was also using 200d embeddings with F1 score on test set at 0.6538. But this model is severely overfit. The result using the 100d embeddings and single RNN layer with the embedding layer frozen are very close at 0.6527 and the model is not severely overfit with training F1 at 0.8861. This model is the one I would choose for runnung inferences if needed as I would expect this one to generalize better.\n",
    "\n",
    "I also found that since the models overfit severely, reducing the layers in the LSTM led to lower f1 on train set but improved performance in the test set. Since the challenge here is not that the model is not able to learn but that the data in train and test set is too disjoint, reducing the size of the model in this way helped to achieve better test performance.\n",
    "\n",
    "I also tried to change the bidirectional LSTM to uni-directional to reduce some overfitting, but this resulted in poor performance across the board, so I have not included those results. I also tried a lot of models with the embeddings layer unfrozen, but the performance was below the benchmark in most cases, so I have only included a select few examples to show that the model performs poorly. I found that a single layer bi-directional LSTM with close to 16 hidden units, a learning rate or 0.001 and no dropouts resulted in consistent performace at f1 score of about 0.64 to 0.65 on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Irony Detection results](irony_detection_results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
